# Story 2.1: Sentence-Transformers Setup

**Story ID:** 2.1
**Assigned to:** @dev
**Sprint:** Sprint 2 (Week 3)
**Status:** Draft
**Estimation:** 5 story points (2 days)

---

## Summary

Set up sentence-transformers library for generating semantic embeddings of decision statements. This enables semantic search capabilities by converting text into 384-dimensional vectors that capture meaning.

---

## Acceptance Criteria

- [ ] sentence-transformers library installed (`pip install sentence-transformers`)
- [ ] Model downloaded and cached: `all-MiniLM-L6-v2`
- [ ] Model loads successfully on startup
- [ ] Embedding generation working
  - [ ] Single decision: <100ms latency
  - [ ] Batch processing: 10+ decisions at once
  - [ ] Output: 384-dimensional vectors
- [ ] Edge cases handled:
  - [ ] Very long text (>512 tokens) truncated properly
  - [ ] Special characters/Unicode preserved
  - [ ] Empty/null text handled gracefully
- [ ] Model metadata validated:
  - [ ] Size: ~22MB disk space
  - [ ] License: MIT (commercial use OK)
  - [ ] Cost: $0 (runs locally, no API calls)
- [ ] Tests passing: `pytest tests/unit/test_embeddings.py` (80%+ coverage)

---

## Tasks

### Task 1: Install and Configure sentence-transformers (0.5 days)

1. **Add dependency to requirements**
   - Add `sentence-transformers==2.3.1` to `requirements.txt`
   - Add `torch==2.1.0` (required dependency)
   - Run `pip install -r requirements.txt`

2. **Create embeddings utility module**
   - Create `app/utils/embeddings.py`
   - Initialize model as singleton
   - Add model loading with caching

3. **Test installation**
   - Verify model downloads (~22MB)
   - Check model loads without errors
   - Validate output dimensions (384)

### Task 2: Implement Single Embedding Generation (0.5 days)

1. **Create embed_text() function**
   - Input: Text string
   - Output: List[float] (384 dimensions)
   - Handle edge cases (empty, null, very long)
   - Measure latency (<100ms target)

2. **Add text preprocessing**
   - Concatenate decision fields: `statement + why + impacts`
   - Truncate to max_length=512 tokens
   - Strip extra whitespace
   - Preserve Unicode characters

3. **Test single embedding**
   - Test with sample decision text
   - Validate output shape (384,)
   - Check latency <100ms
   - Test with edge cases

### Task 3: Implement Batch Embedding (0.5 days)

1. **Create embed_batch() function**
   - Input: List[str] (multiple texts)
   - Output: List[List[float]] (multiple vectors)
   - Use `model.encode(texts, batch_size=32)`
   - Optimize for throughput

2. **Configure batch processing**
   - Batch size: 32 (optimal for MiniLM)
   - Show progress bar for large batches
   - Handle mixed lengths in batch

3. **Test batch processing**
   - Test with 10, 50, 100 decisions
   - Measure throughput improvement
   - Verify output consistency

### Task 4: Handle Edge Cases (0.25 days)

1. **Implement error handling**
   - Empty text â†’ return zero vector or raise exception
   - Null text â†’ raise ValueError
   - Very long text (>512 tokens) â†’ truncate with warning
   - Special characters â†’ preserve (model handles Unicode)

2. **Add logging**
   - Log model load time
   - Log embedding latency metrics
   - Warn on truncation

3. **Test edge cases**
   - Empty string
   - Very long text (2000+ tokens)
   - Unicode/emoji text
   - HTML tags/special chars

### Task 5: Write Tests (0.25 days)

1. **Create test file: `tests/unit/test_embeddings.py`**
   - Test model loading
   - Test single embedding shape
   - Test batch embedding
   - Test latency <100ms
   - Test edge cases
   - Test text preprocessing

2. **Run tests**
   - `pytest tests/unit/test_embeddings.py --cov`
   - Target: 80%+ coverage
   - All tests passing

---

## Dev Notes

### Model Selection: all-MiniLM-L6-v2

**Why this model?**
- **Speed**: <100ms per embedding (fast for real-time use)
- **Quality**: 384-dim vectors capture semantic meaning well
- **Size**: Only 22MB (can cache locally)
- **License**: MIT (commercial use allowed)
- **Cost**: Free (runs locally, no API calls)
- **Support**: Widely used, well-documented

**Alternative models considered:**
- `all-mpnet-base-v2`: Higher quality (768-dim), but slower (300ms+)
- `paraphrase-MiniLM-L3-v2`: Faster (50ms), but lower quality
- OpenAI embeddings: High quality, but costs $0.0001/1K tokens

### Implementation Code

```python
# app/utils/embeddings.py
from sentence_transformers import SentenceTransformer
from typing import List, Union
import logging

logger = logging.getLogger(__name__)

class EmbeddingService:
    """Singleton service for generating text embeddings."""

    _instance = None
    _model = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if self._model is None:
            logger.info("Loading sentence-transformers model: all-MiniLM-L6-v2")
            self._model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("Model loaded successfully")

    def embed_text(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.

        Args:
            text: Input text (decision statement + context)

        Returns:
            384-dimensional embedding vector

        Raises:
            ValueError: If text is None or empty
        """
        if not text or text.strip() == "":
            raise ValueError("Text cannot be empty")

        # Truncate to max 512 tokens (model limit)
        if len(text) > 2000:  # Rough token estimate
            logger.warning(f"Text too long ({len(text)} chars), truncating")
            text = text[:2000]

        embedding = self._model.encode(text, convert_to_numpy=True)
        return embedding.tolist()

    def embed_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        Generate embeddings for multiple texts.

        Args:
            texts: List of input texts
            batch_size: Batch size for processing (default 32)

        Returns:
            List of 384-dimensional embedding vectors
        """
        if not texts:
            return []

        # Preprocess texts
        processed = [t[:2000] if len(t) > 2000 else t for t in texts]

        embeddings = self._model.encode(
            processed,
            batch_size=batch_size,
            show_progress_bar=len(texts) > 100,
            convert_to_numpy=True
        )
        return embeddings.tolist()

# Usage example
embedder = EmbeddingService()

# Single embedding
decision_text = f"{decision.statement} {decision.why} {' '.join(decision.impacts)}"
embedding = embedder.embed_text(decision_text)
print(f"Embedding shape: {len(embedding)}")  # 384

# Batch embedding
texts = [f"{d.statement} {d.why}" for d in decisions]
embeddings = embedder.embed_batch(texts)
print(f"Generated {len(embeddings)} embeddings")
```

### Performance Benchmarks

| Operation | Latency | Throughput |
|-----------|---------|------------|
| Single embedding | <100ms | 10+ per second |
| Batch (32 decisions) | ~500ms | 60+ per second |
| Batch (100 decisions) | ~1.5s | 65+ per second |

### Memory Usage

- Model size: ~22MB RAM
- Per embedding: ~1.5KB (384 floats)
- 1000 embeddings: ~1.5MB RAM

---

## File List

**Modified/Created:**
- `requirements.txt` (modify) - Add sentence-transformers dependency
- `app/utils/embeddings.py` (new) - Embedding service implementation
- `tests/unit/test_embeddings.py` (new) - Unit tests
- `app/config.py` (modify) - Add embedding config (optional)

---

## Testing Strategy

### Unit Tests (`tests/unit/test_embeddings.py`)

```python
import pytest
from app.utils.embeddings import EmbeddingService

def test_model_loads():
    """Test model initializes successfully."""
    embedder = EmbeddingService()
    assert embedder._model is not None

def test_single_embedding_shape():
    """Test embedding has correct dimensions."""
    embedder = EmbeddingService()
    text = "Foundation depth changed from 2m to 3m due to soil analysis"
    embedding = embedder.embed_text(text)
    assert len(embedding) == 384
    assert all(isinstance(x, float) for x in embedding)

def test_batch_embedding():
    """Test batch processing works correctly."""
    embedder = EmbeddingService()
    texts = [
        "Decision about MEP systems",
        "Architectural layout modified",
        "Budget increased by 10%"
    ]
    embeddings = embedder.embed_batch(texts)
    assert len(embeddings) == 3
    assert all(len(emb) == 384 for emb in embeddings)

def test_empty_text_raises_error():
    """Test empty text handling."""
    embedder = EmbeddingService()
    with pytest.raises(ValueError):
        embedder.embed_text("")

def test_long_text_truncation():
    """Test very long text is truncated."""
    embedder = EmbeddingService()
    long_text = "x" * 3000
    embedding = embedder.embed_text(long_text)
    assert len(embedding) == 384  # Should still work

def test_unicode_handling():
    """Test Unicode characters preserved."""
    embedder = EmbeddingService()
    text = "DecisÃ£o sobre fundaÃ§Ã£o ðŸ—ï¸"
    embedding = embedder.embed_text(text)
    assert len(embedding) == 384

def test_embedding_latency():
    """Test embedding generation is fast."""
    import time
    embedder = EmbeddingService()
    text = "Foundation depth changed from 2m to 3m"

    start = time.time()
    embedding = embedder.embed_text(text)
    duration = time.time() - start

    assert duration < 0.1  # <100ms
    assert len(embedding) == 384
```

### Coverage Target

- Minimum: 80%
- Target: 90%+
- Run: `pytest tests/unit/test_embeddings.py --cov=app.utils.embeddings --cov-report=html`

---

## Change Log

| Date | Change |
|------|--------|
| 2026-02-08 | Created story |

---

**Related Stories:** 2.2 (pgvector setup), 2.3 (embedding pipeline)
**Blocked By:** None
**Blocks:** 2.3 (needs embedding service working)
