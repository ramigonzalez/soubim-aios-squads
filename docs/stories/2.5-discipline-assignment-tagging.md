# Story 2.5: Discipline Assignment & Tagging

**Story ID:** 2.5
**Assigned to:** @dev
**Sprint:** Sprint 3 (Week 5)
**Status:** Draft
**Estimation:** 3 story points (1.5 days)

---

## Summary

Enhance the LLM extraction prompt to automatically infer and assign the primary discipline for each decision. This enables discipline-based filtering and analytics in the dashboard.

---

## Acceptance Criteria

- [ ] Discipline automatically assigned during decision extraction
- [ ] Valid disciplines: `architecture`, `mep`, `landscape`, `interior`, `electrical`, `plumbing`, `structural`, `general`
- [ ] Discipline stored in `decisions.discipline` column
- [ ] Database enum constraint enforced (no invalid values)
- [ ] Multi-discipline decisions: Assign single primary discipline
- [ ] Manual audit: 90%+ accuracy on 30 test decisions
- [ ] Fallback: `general` if discipline cannot be determined
- [ ] Tests passing: `pytest tests/unit/test_discipline_assignment.py` (80%+ coverage)

---

## Tasks

### Task 1: Update LLM Extraction Prompt (0.5 days)

1. **Modify extraction prompt template**
   ```python
   # app/agents/prompts/extraction.py

   DISCIPLINE_GUIDELINES = """
   For each decision, determine the PRIMARY discipline affected:

   - architecture: Building layout, structural design, material choices, spatial planning, aesthetic decisions
   - mep: HVAC systems, mechanical systems, plumbing systems, general MEP coordination
   - landscape: Site design, outdoor elements, landscaping, hardscaping, exterior features
   - interior: Interior finishes, interior layout, interior design decisions, furniture
   - electrical: Electrical systems, power distribution, lighting systems, electrical panels
   - plumbing: Plumbing systems, water supply, drainage, fixtures
   - structural: Structural engineering, load-bearing elements, foundations, reinforcement
   - general: Cross-disciplinary, administrative, or unclear discipline

   If multiple disciplines are affected, choose the PRIMARY one most impacted.
   """

   EXTRACTION_PROMPT = f"""
   Extract all architectural decisions from this transcript.

   For each decision, provide:
   1. decision_statement: One-line summary
   2. who: Person who made the decision
   3. timestamp: Time in transcript (HH:MM:SS)
   4. discipline: Primary discipline affected
   5. why: Full reasoning (2-3 sentences)
   6. causation: What triggered this decision
   7. impacts: List of impacts (timeline, budget, scope)
   8. consensus: Votes by discipline (AGREE/DISAGREE/SILENT)
   9. confidence: 0.0-1.0 confidence score

   {DISCIPLINE_GUIDELINES}

   Respond in JSON format:
   {{
     "decisions": [
       {{
         "decision_statement": "...",
         "who": "...",
         "timestamp": "HH:MM:SS",
         "discipline": "architecture",
         ...
       }}
     ]
   }}
   """
   ```

2. **Add examples to prompt**
   ```python
   EXAMPLES = """
   Examples:
   - "Changed building facade material" → architecture
   - "Increased HVAC capacity" → mep
   - "Added trees to courtyard" → landscape
   - "Selected lobby floor finish" → interior
   - "Upgraded electrical panel" → electrical
   - "Relocated water main" → plumbing
   - "Reinforced foundation" → structural
   - "Extended project timeline" → general
   """
   ```

3. **Test prompt with sample transcript**

### Task 2: Add Discipline Validation (0.25 days)

1. **Validate discipline in extraction pipeline**
   ```python
   # app/services/extraction.py

   VALID_DISCIPLINES = [
       'architecture', 'mep', 'landscape', 'interior',
       'electrical', 'plumbing', 'structural', 'general'
   ]

   def validate_and_normalize_discipline(discipline: str) -> str:
       """Validate and normalize discipline value."""
       normalized = discipline.lower().strip()

       if normalized not in VALID_DISCIPLINES:
           logger.warning(f"Invalid discipline '{discipline}', defaulting to 'general'")
           return 'general'

       return normalized

   # In extraction function
   for decision in extracted_decisions:
       decision['discipline'] = validate_and_normalize_discipline(
           decision.get('discipline', 'general')
       )
   ```

2. **Add database constraint check**
   - Verify CHECK constraint exists in decisions table
   - Test constraint enforcement

3. **Add fallback logic**
   - If LLM doesn't return discipline → default to `general`
   - Log warning when fallback used

### Task 3: Manual Audit and Accuracy Testing (0.5 days)

1. **Create audit script**
   ```python
   # scripts/audit_disciplines.py

   import random
   from app.database import Session
   from app.database.models import Decision

   def audit_discipline_accuracy(sample_size=30):
       """Manually audit discipline assignments."""
       db = Session()

       # Get random sample of decisions
       decisions = db.query(Decision).order_by(func.random()).limit(sample_size).all()

       correct = 0
       for i, d in enumerate(decisions, 1):
           print(f"\n--- Decision {i}/{sample_size} ---")
           print(f"Statement: {d.statement}")
           print(f"Why: {d.why}")
           print(f"Assigned Discipline: {d.discipline}")

           user_input = input("Is this correct? (y/n/skip): ").lower()

           if user_input == 'y':
               correct += 1
           elif user_input == 'skip':
               continue

       accuracy = (correct / sample_size) * 100
       print(f"\n✅ Accuracy: {accuracy:.1f}% ({correct}/{sample_size})")
       return accuracy

   if __name__ == '__main__':
       accuracy = audit_discipline_accuracy()
       assert accuracy >= 90.0, f"Accuracy {accuracy:.1f}% below 90% threshold"
   ```

2. **Run audit on test dataset**
   - Sample 30 decisions
   - Manually verify each discipline
   - Target: 90%+ accuracy

3. **Adjust prompt if needed**
   - If accuracy <90%, refine discipline guidelines
   - Add more examples to prompt
   - Re-test until target met

### Task 4: Add Analytics Support (0.25 days)

1. **Create discipline distribution query**
   ```python
   # app/api/projects.py

   @router.get("/projects/{project_id}/stats/disciplines")
   async def get_discipline_stats(project_id: str, current_user: User = Depends(get_current_user)):
       """Get distribution of decisions by discipline."""
       db = Session()

       stats = db.query(
           Decision.discipline,
           func.count(Decision.id).label('count')
       ).filter(
           Decision.project_id == project_id
       ).group_by(
           Decision.discipline
       ).all()

       return {
           'project_id': project_id,
           'disciplines': {row.discipline: row.count for row in stats}
       }
   ```

2. **Add to project detail endpoint**
   - Include discipline breakdown in project stats
   - Show in dashboard

### Task 5: Write Tests (0.5 days)

1. **Create test file: `tests/unit/test_discipline_assignment.py`**
   ```python
   import pytest
   from app.services.extraction import validate_and_normalize_discipline

   def test_valid_disciplines_accepted():
       """Test all valid disciplines are accepted."""
       valid = ['architecture', 'mep', 'landscape', 'interior',
                'electrical', 'plumbing', 'structural', 'general']

       for discipline in valid:
           result = validate_and_normalize_discipline(discipline)
           assert result == discipline

   def test_invalid_discipline_defaults_to_general():
       """Test invalid discipline defaults to general."""
       invalid = ['foo', 'bar', '', 'mechanical']

       for discipline in invalid:
           result = validate_and_normalize_discipline(discipline)
           assert result == 'general'

   def test_discipline_normalization():
       """Test discipline values are normalized."""
       assert validate_and_normalize_discipline('ARCHITECTURE') == 'architecture'
       assert validate_and_normalize_discipline(' mep ') == 'mep'

   @pytest.mark.asyncio
   async def test_discipline_extraction_in_pipeline():
       """Test discipline extracted correctly from transcript."""
       transcript = Transcript(
           transcript_text='CARLOS: We need to change the structural design...'
       )

       decisions = await extract_decisions(transcript)
       assert len(decisions) > 0
       assert decisions[0].discipline in VALID_DISCIPLINES

   @pytest.mark.asyncio
   async def test_multi_discipline_assigns_primary():
       """Test multi-discipline decision gets single primary."""
       transcript = Transcript(
           transcript_text='Decision affects both architecture and MEP...'
       )

       decisions = await extract_decisions(transcript)
       decision = decisions[0]

       # Should have exactly one discipline
       assert isinstance(decision.discipline, str)
       assert decision.discipline in VALID_DISCIPLINES
   ```

2. **Run tests**
   - `pytest tests/unit/test_discipline_assignment.py --cov`
   - Target: 80%+ coverage

---

## Dev Notes

### Discipline Definitions

| Discipline | Scope | Examples |
|-----------|-------|----------|
| **architecture** | Building design, layout, aesthetics | Facade materials, floor plans, structural choices |
| **mep** | Mechanical, electrical, plumbing systems | HVAC, general MEP coordination |
| **landscape** | Site and outdoor elements | Trees, hardscaping, outdoor features |
| **interior** | Interior design and finishes | Flooring, wall finishes, furniture |
| **electrical** | Electrical systems | Power, lighting, panels |
| **plumbing** | Plumbing and water systems | Water supply, drainage, fixtures |
| **structural** | Structural engineering | Foundations, load-bearing, reinforcement |
| **general** | Cross-disciplinary or unclear | Timeline, budget, administrative |

### LLM Prompt Enhancement

```python
# Example extraction response with discipline
{
  "decisions": [
    {
      "decision_statement": "Changed foundation depth from 2m to 3m",
      "who": "Carlos (Structural Engineer)",
      "timestamp": "00:23:15",
      "discipline": "structural",  # ← NEW FIELD
      "why": "Soil analysis showed need for deeper foundations",
      "causation": "Geotechnical report findings",
      "impacts": [
        {"type": "timeline", "change": "+2 weeks"},
        {"type": "budget", "change": "+$50K"}
      ],
      "consensus": {
        "architecture": "AGREE",
        "structural": "AGREE",
        "mep": "SILENT"
      },
      "confidence": 0.92
    }
  ]
}
```

### Accuracy Tracking

```python
# Track discipline assignment accuracy
{
  "total_decisions": 127,
  "manual_audit_sample": 30,
  "correct_assignments": 28,
  "accuracy": 0.93,  # 93%
  "common_errors": [
    {"confused": ["electrical", "mep"], "count": 1},
    {"confused": ["interior", "architecture"], "count": 1}
  ]
}
```

---

## File List

**Modified/Created:**
- `app/agents/prompts/extraction.py` (modify) - Add discipline guidelines to prompt
- `app/services/extraction.py` (modify) - Add discipline validation
- `app/api/projects.py` (modify) - Add discipline stats endpoint
- `scripts/audit_disciplines.py` (new) - Manual audit script
- `tests/unit/test_discipline_assignment.py` (new) - Discipline tests

---

## Testing Strategy

### Unit Tests

```python
@pytest.mark.asyncio
async def test_discipline_inference_from_context():
    """Test LLM correctly infers discipline from decision context."""
    test_cases = [
        {
            'statement': 'Changed building facade material from glass to stone',
            'why': 'Client preferred stone aesthetic',
            'expected_discipline': 'architecture'
        },
        {
            'statement': 'Increased HVAC capacity by 20%',
            'why': 'Larger occupancy than originally planned',
            'expected_discipline': 'mep'
        },
        {
            'statement': 'Added reinforcement to foundation',
            'why': 'Increased load from taller building',
            'expected_discipline': 'structural'
        }
    ]

    for case in test_cases:
        transcript = create_test_transcript(case['statement'], case['why'])
        decisions = await extract_decisions(transcript)

        assert len(decisions) == 1
        assert decisions[0].discipline == case['expected_discipline']

@pytest.mark.asyncio
async def test_database_constraint_enforces_valid_disciplines():
    """Test database rejects invalid disciplines."""
    decision = Decision(
        statement='Test decision',
        discipline='invalid_discipline',  # Should fail
        who='Test User',
        timestamp='00:00:00'
    )

    with pytest.raises(IntegrityError):
        db.add(decision)
        db.commit()
```

### Manual Audit Process

1. Extract decisions from 5 test transcripts
2. Randomly sample 30 decisions
3. For each decision:
   - Read statement and context
   - Verify assigned discipline is correct
   - Record result (correct/incorrect)
4. Calculate accuracy
5. If <90%, refine prompt and re-test

---

## Change Log

| Date | Change |
|------|--------|
| 2026-02-08 | Created story |

---

**Related Stories:** 2.3 (Extraction pipeline), 2.4 (Search filtering by discipline)
**Blocked By:** 2.3 (needs extraction pipeline working)
**Blocks:** 3.6 (frontend filtering needs discipline data)
