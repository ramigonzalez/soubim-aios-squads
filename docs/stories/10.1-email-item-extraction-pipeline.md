# Story 10.1: Email Item Extraction Pipeline

**Story ID:** 10.1
**Epic:** E10 — Email & Document Integration
**Assigned to:** @dev
**Sprint:** Sprint 7 (Week 11)
**Status:** Complete
**Estimation:** 8 story points (3-4 days)
**Review:** @pm (Morgan) — prompt review for extraction accuracy
**Architecture:** `docs/architecture/04-AGENT-PIPELINE.md`

---

## Summary

Build the AI extraction pipeline for email sources. When an approved email source is processed, the system extracts project items (all 5 types) from the email body using a dedicated prompt file (`extract_email.md`). The pipeline handles email thread deduplication (avoids re-extracting from quoted replies), receives `ProjectParticipant[]` roster for discipline inference, and stores extracted items with `source_type='email'` and `source_id` linked to the originating email source.

---

## Acceptance Criteria

### Email Extraction Prompt
- [x] **Create `decision-log-backend/app/prompts/extract_email.md`**
- [x] Prompt adapted for email content structure (vs. meeting transcript structure)
- [x] Instructs LLM to:
  - [x] Extract all 5 item types: idea, topic, decision, action_item, information
  - [x] Identify `who` from email sender and referenced names
  - [x] Map speakers to disciplines using `ProjectParticipant[]` roster
  - [x] Populate `affected_disciplines[]` per item following inference guidelines from PRD
  - [x] Set `owner` for action items when mentioned
  - [x] Handle email-specific patterns:
    - [x] "Please review..." → action_item
    - [x] "We've decided..." → decision
    - [x] "FYI..." → information
    - [x] "What if..." → idea
    - [x] "We need to discuss..." → topic
- [x] Prompt explicitly instructs: ignore quoted replies (text after `>` markers or `---Original Message---`)
- [x] Prompt receives: `email_body`, `email_subject`, `email_from`, `email_to`, `participants[]`

### Email Extraction Service
- [x] **Create `decision-log-backend/app/services/email_extractor.py`**
- [x] Loads prompt from `app/prompts/extract_email.md` at runtime
- [x] Strips quoted replies from email body before extraction:
  ```python
  def strip_quoted_replies(body: str) -> str:
      """Remove quoted text from email body."""
      lines = body.split('\n')
      clean_lines = []
      for line in lines:
          if line.startswith('>') or line.startswith('On ') and ' wrote:' in line:
              break  # Stop at first quoted section
          if '---Original Message---' in line or '---------- Forwarded message' in line:
              break
          clean_lines.append(line)
      return '\n'.join(clean_lines)
  ```
- [x] Calls Claude API with formatted prompt + cleaned email body
- [x] Parses JSON response into `ProjectItem` records
- [x] Each extracted item has:
  - [x] `source_type='email'`
  - [x] `source_id` linked to the email `Source` record
  - [x] `item_type` classified by LLM
  - [x] `affected_disciplines[]` inferred from participants and content
  - [x] `statement`, `who`, `timestamp` (email date as timestamp)
- [ ] Generates vector embeddings for each extracted item
- [ ] Runs agent enrichment pipeline (similarity check, confidence, anomaly detection)

### Pipeline Integration
- [x] Email extraction triggered when email source status changes to `approved` (via Story 7.1 approval flow)
- [ ] Background task execution (FastAPI `BackgroundTasks`)
- [x] Source status updated to `processed` after successful extraction
- [x] Error handling: if extraction fails, source stays `approved`, error logged
- [ ] Re-processing support: admin can re-trigger extraction for a processed email

### Thread Deduplication
- [x] When processing an email from a thread:
  - [x] Check if other emails from the same `email_thread_id` have already been processed
  - [x] Strip quoted content to avoid extracting the same items from replies
  - [x] Log warning if potential overlap detected
- [x] Each email in a thread is processed independently (new items only from the new content)

### Extraction Accuracy
- [ ] Target: 85%+ accuracy on item type classification (measured against test corpus)
- [x] Test corpus: at least 5 curated email samples with known expected items
- [ ] Accuracy measured as: correct item type / total extracted items

---

## Tasks

### Task 1: Create Email Extraction Prompt (0.5 days)

1. **Create `decision-log-backend/app/prompts/extract_email.md`**
   ```markdown
   # Email Item Extraction Prompt

   You are analyzing a project email to extract structured project items.

   ## Context
   - Project: {{project_name}}
   - Email Subject: {{email_subject}}
   - Email From: {{email_from}}
   - Email To: {{email_to}}
   - Date: {{email_date}}

   ## Project Participants
   {{#each participants}}
   - {{name}} ({{email}}) — Discipline: {{discipline}}
   {{/each}}

   ## Instructions
   Extract ALL valuable project items from the email body below.
   Classify each item as one of: idea, topic, decision, action_item, information

   ### Classification Signals
   - **idea**: "what if", "we could try", "maybe consider", proposals
   - **topic**: "we need to discuss", "still evaluating", "pending review"
   - **decision**: "we agreed", "decided", "confirmed", "approved"
   - **action_item**: "X will...", "need to prepare", "by Friday", assignments
   - **information**: "FYI", "for reference", "the permit was approved", facts/updates

   ### Rules
   1. IGNORE quoted replies (text after > markers or ---Original Message---)
   2. Map mentioned names to disciplines using the participant list
   3. For action_items, identify the owner (person responsible)
   4. Set affected_disciplines[] based on who is involved and what areas are impacted
   5. Extract only from the NEW content of this email

   ## Email Body
   {{email_body}}

   ## Output Format
   Return a JSON array of items:
   ```json
   [
     {
       "item_type": "decision",
       "statement": "Agreed to use LED lighting with smart controls",
       "who": "Gabriela",
       "affected_disciplines": ["electrical", "architecture"],
       "context": "Discussed in thread about lighting options",
       "owner": null,
       "due_date": null
     }
   ]
   ```
   ```

### Task 2: Create Email Extractor Service (1 day)

1. **Create `decision-log-backend/app/services/email_extractor.py`**
   ```python
   import json
   from pathlib import Path
   from app.services.llm_client import call_claude
   from app.database.models import ProjectItem, Source, ProjectParticipant
   from app.services.embedding import generate_embedding

   PROMPT_PATH = Path(__file__).parent.parent / "prompts" / "extract_email.md"

   class EmailExtractor:
       def __init__(self, db_session):
           self.db = db_session
           self.prompt_template = PROMPT_PATH.read_text()

       def extract(self, source: Source, participants: list[ProjectParticipant]) -> list[ProjectItem]:
           # Strip quoted replies
           clean_body = strip_quoted_replies(source.raw_content or "")

           # Build prompt
           prompt = self.prompt_template
           prompt = prompt.replace("{{project_name}}", source.project.title)
           prompt = prompt.replace("{{email_subject}}", source.title or "")
           prompt = prompt.replace("{{email_from}}", source.email_from or "")
           prompt = prompt.replace("{{email_to}}", json.dumps(source.email_to or []))
           prompt = prompt.replace("{{email_date}}", str(source.occurred_at))
           prompt = prompt.replace("{{email_body}}", clean_body)

           # Format participants
           participant_text = "\n".join(
               f"- {p.name} ({p.email}) — Discipline: {p.discipline}"
               for p in participants
           )
           prompt = prompt.replace("{{#each participants}}\n- {{name}} ({{email}}) — Discipline: {{discipline}}\n{{/each}}", participant_text)

           # Call LLM
           response = call_claude(prompt)
           raw_items = json.loads(response)

           # Create ProjectItem records
           items = []
           for raw in raw_items:
               item = ProjectItem(
                   project_id=source.project_id,
                   source_id=source.id,
                   source_type="email",
                   item_type=raw["item_type"],
                   statement=raw["statement"],
                   who=raw.get("who"),
                   affected_disciplines=raw.get("affected_disciplines", []),
                   owner=raw.get("owner"),
                   timestamp=source.occurred_at,
               )
               item.embedding = generate_embedding(item.statement)
               items.append(item)

           return items
   ```

### Task 3: Integrate with Ingestion Pipeline (0.5 days)

1. **Modify `decision-log-backend/app/services/ingestion_processor.py`** (or equivalent)
   ```python
   async def process_approved_source(source_id: str, db: Session):
       source = db.query(Source).get(source_id)
       participants = db.query(ProjectParticipant).filter_by(project_id=source.project_id).all()

       if source.source_type == "meeting":
           # Existing meeting extraction
           items = meeting_extractor.extract(source, participants)
       elif source.source_type == "email":
           extractor = EmailExtractor(db)
           items = extractor.extract(source, participants)
       # ... future: document

       db.add_all(items)
       source.ingestion_status = "processed"
       db.commit()
   ```

### Task 4: Thread Deduplication Logic (0.5 days)

1. **Add deduplication check in `EmailExtractor`**
   ```python
   def check_thread_overlap(self, source: Source) -> bool:
       """Check if other emails in this thread were already processed."""
       if not source.email_thread_id:
           return False
       processed = self.db.query(Source).filter(
           Source.email_thread_id == source.email_thread_id,
           Source.id != source.id,
           Source.ingestion_status == "processed",
       ).count()
       return processed > 0
   ```

### Task 5: Create Test Corpus & Tests (1 day)

1. **Create `decision-log-backend/tests/fixtures/emails/` directory**
   - `test_email_decision.txt` — email with clear decisions
   - `test_email_action_items.txt` — email with action items and owners
   - `test_email_thread.txt` — email with quoted reply (test stripping)
   - `test_email_mixed.txt` — email with multiple item types
   - `test_email_fyi.txt` — informational email

2. **Create `decision-log-backend/tests/test_email_extractor.py`**
   - Quoted reply stripping removes quoted text
   - Extraction produces items with correct types
   - Participant discipline mapping works
   - Thread deduplication detects overlap
   - Source status updated to processed
   - Failed extraction keeps source as approved
   - Empty email body produces no items

---

## Dev Notes

### Email Content vs Meeting Transcript

| Aspect | Meeting Transcript | Email Body |
|--------|-------------------|------------|
| Length | 5K-20K tokens | 100-2K tokens |
| Structure | Timestamped dialogue | Prose paragraphs |
| Speakers | Multiple, identified | Sender + references |
| Thread context | Single meeting | Reply chain |
| Item density | 30-50 items | 1-10 items |

The email prompt must be adapted for shorter, denser content with fewer speakers.

### Quoted Reply Patterns

Common patterns to strip:
- Lines starting with `>`
- Lines starting with `On ... wrote:`
- `---Original Message---`
- `---------- Forwarded message ----------`
- Gmail-style: `On Mon, Feb 10, 2026 at 3:45 PM John <john@email.com> wrote:`

### Prompt File Loading

Following the pattern established in Story 5.4:
- Prompts stored in `app/prompts/` directory
- Loaded at runtime via `Path.read_text()`
- Template variables replaced with actual values
- Version-controlled for iteration

---

## File List

**New Files:**
- `decision-log-backend/app/prompts/extract_email.md` — Email extraction prompt
- `decision-log-backend/app/services/email_extractor.py` — Email extraction service
- `decision-log-backend/app/services/ingestion_pipeline.py` — Ingestion pipeline with email/meeting dispatch
- `decision-log-backend/tests/test_email_extractor.py` — 24 unit tests
- `decision-log-backend/tests/fixtures/emails/test_email_decision.txt`
- `decision-log-backend/tests/fixtures/emails/test_email_action_items.txt`
- `decision-log-backend/tests/fixtures/emails/test_email_thread.txt`
- `decision-log-backend/tests/fixtures/emails/test_email_mixed.txt`
- `decision-log-backend/tests/fixtures/emails/test_email_fyi.txt`

**Modified Files:**
- None (ingestion_pipeline.py created new — no pre-existing file)

---

## Testing Strategy

### Unit Tests
```
- ✅ strip_quoted_replies removes > quoted text
- ✅ strip_quoted_replies removes ---Original Message---
- ✅ strip_quoted_replies preserves original content
- ✅ Extraction produces items with correct item_type
- ✅ Participant mapping assigns correct disciplines
- ✅ Action items have owner populated
- ✅ Thread deduplication detects processed siblings
- ✅ Source status transitions: approved → processed
- ✅ Failed extraction keeps source as approved
- ✅ Empty email body produces no items
- ✅ Email with only quoted text produces no items
```

### Accuracy Testing
- Run extraction on 5 curated email samples
- Compare extracted items against expected items
- Target: 85%+ type classification accuracy

### Coverage Target: 80%+

---

## Change Log

| Date | Change |
|------|--------|
| 2026-02-20 | Created story |
| 2026-02-27 | Implemented: prompt, extractor, pipeline, tests (24 passing), wave log |

---

**Related Stories:** 5.4 (AI prompt evolution), 7.1 (ingestion pipeline), 7.4 (Gmail poller provides email sources)
**Blocked By:** 7.1 (Source entity and approval flow), 5.4 (extraction prompt pattern)
**Blocks:** 10.2 (document extraction follows same pattern)
