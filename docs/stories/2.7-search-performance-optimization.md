# Story 2.7: Search Performance Optimization

**Story ID:** 2.7
**Assigned to:** @dev
**Sprint:** Sprint 3 (Week 5)
**Status:** Draft
**Estimation:** 5 story points (2 days)

---

## Summary

Optimize vector search performance to handle growing dataset size. Implement IVFFLAT index, query optimization, and caching strategies to maintain <500ms query times even with thousands of decisions.

---

## Acceptance Criteria

- [ ] IVFFLAT index created on `decisions.embedding` column
- [ ] Index parameters tuned: `lists=100` for MVP
- [ ] Query performance <500ms for 1000 decisions
- [ ] Query performance <1s for 10,000 decisions
- [ ] Query plan analysis: Index usage verified
- [ ] Query caching implemented (optional for MVP)
- [ ] Monitoring: Query time metrics logged
- [ ] Load test: 100 concurrent searches supported
- [ ] Documentation: Performance tuning guide created
- [ ] Tests passing: `pytest tests/performance/test_search_optimization.py`

---

## Tasks

### Task 1: Create IVFFLAT Index (0.5 days)

1. **Create Alembic migration**
   ```python
   # alembic/versions/003_add_ivfflat_index.py
   """Add IVFFLAT index for vector search

   Revision ID: 003
   Revises: 002
   Create Date: 2026-02-XX
   """
   from alembic import op

   def upgrade():
       # Note: Run AFTER decisions table has data
       # Index build time: ~30s for 1000 decisions

       op.execute("""
           CREATE INDEX IF NOT EXISTS idx_decisions_embedding_cosine
           ON decisions
           USING ivfflat (embedding vector_cosine_ops)
           WITH (lists = 100);
       """)

       # Analyze table for optimal query planning
       op.execute("ANALYZE decisions;")

   def downgrade():
       op.execute("DROP INDEX IF EXISTS idx_decisions_embedding_cosine;")
   ```

2. **Apply migration**
   ```bash
   alembic upgrade head
   ```

3. **Verify index created**
   ```sql
   SELECT indexname, indexdef
   FROM pg_indexes
   WHERE tablename = 'decisions'
   AND indexname = 'idx_decisions_embedding_cosine';
   ```

### Task 2: Optimize Query Plans (0.75 days)

1. **Analyze current query performance**
   ```python
   # scripts/analyze_query_performance.py

   import time
   from sqlalchemy import text

   def analyze_vector_search_query():
       """Analyze query execution plan."""
       db = Session()

       query_emb = [0.5] * 384

       # Get query plan
       explain_sql = text("""
           EXPLAIN ANALYZE
           SELECT id, decision_statement,
                  1 - (embedding <=> :query_emb::vector) as similarity
           FROM decisions
           WHERE project_id = :project_id
               AND embedding IS NOT NULL
           ORDER BY embedding <=> :query_emb::vector
           LIMIT 10;
       """)

       result = db.execute(explain_sql, {
           'query_emb': query_emb,
           'project_id': 'test-project-id'
       }).fetchall()

       for row in result:
           print(row[0])

   if __name__ == '__main__':
       analyze_vector_search_query()
   ```

2. **Verify index usage**
   - Check EXPLAIN output shows "Index Scan using idx_decisions_embedding_cosine"
   - Not "Seq Scan on decisions"

3. **Tune index parameters**
   ```sql
   -- For larger datasets, adjust lists parameter
   -- Rule: lists ≈ sqrt(total_rows)
   -- 1000 rows → lists = 32
   -- 10000 rows → lists = 100
   -- 100000 rows → lists = 316
   ```

### Task 3: Add Query Caching (0.5 days)

1. **Implement embedding cache**
   ```python
   # app/utils/query_cache.py

   from functools import lru_cache
   from hashlib import sha256
   import json

   @lru_cache(maxsize=1000)
   def get_cached_query_embedding(query_text: str) -> tuple:
       """
       Cache embeddings for repeated queries.

       Returns tuple so it's hashable for lru_cache.
       """
       from app.utils.embeddings import EmbeddingService

       embedder = EmbeddingService()
       embedding = embedder.embed_text(query_text)

       return tuple(embedding)  # Convert list to tuple for caching

   def clear_embedding_cache():
       """Clear embedding cache (for tests or memory management)."""
       get_cached_query_embedding.cache_clear()
   ```

2. **Use cached embeddings in search endpoint**
   ```python
   # app/api/decisions.py

   from app.utils.query_cache import get_cached_query_embedding

   @router.get("/projects/{project_id}/decisions/search")
   async def search_decisions(q: str, ...):
       # Use cached embedding if available
       query_embedding = list(get_cached_query_embedding(q))

       # Rest of search logic...
   ```

3. **Add cache monitoring**
   ```python
   @router.get("/admin/cache/stats")
   async def get_cache_stats():
       """Get cache statistics."""
       info = get_cached_query_embedding.cache_info()

       return {
           'hits': info.hits,
           'misses': info.misses,
           'size': info.currsize,
           'maxsize': info.maxsize,
           'hit_rate': info.hits / (info.hits + info.misses) if info.hits + info.misses > 0 else 0
       }
   ```

### Task 4: Add Performance Monitoring (0.5 days)

1. **Log query metrics**
   ```python
   # app/api/decisions.py

   import time
   import logging

   logger = logging.getLogger(__name__)

   @router.get("/projects/{project_id}/decisions/search")
   async def search_decisions(...):
       start = time.time()

       # Generate embedding
       emb_start = time.time()
       query_embedding = embedder.embed_text(q)
       emb_time = time.time() - emb_start

       # Execute search
       search_start = time.time()
       results = db.execute(sql, params).fetchall()
       search_time = time.time() - search_start

       total_time = time.time() - start

       # Log metrics
       logger.info(
           f"Search query: '{q}' | "
           f"Embedding: {emb_time*1000:.0f}ms | "
           f"Search: {search_time*1000:.0f}ms | "
           f"Total: {total_time*1000:.0f}ms | "
           f"Results: {len(results)}"
       )

       return {..., 'performance': {
           'embedding_ms': round(emb_time * 1000),
           'search_ms': round(search_time * 1000),
           'total_ms': round(total_time * 1000)
       }}
   ```

2. **Add Prometheus metrics (optional)**
   ```python
   from prometheus_client import Histogram

   vector_search_latency = Histogram(
       'vector_search_latency_seconds',
       'Vector search query latency'
   )

   @vector_search_latency.time()
   def execute_vector_search(...):
       # Search logic
       pass
   ```

### Task 5: Load Testing (0.75 days)

1. **Create load test script**
   ```python
   # tests/load/test_concurrent_searches.py

   import asyncio
   import time
   import random
   from concurrent.futures import ThreadPoolExecutor

   async def simulate_search(query: str):
       """Simulate single search request."""
       response = await client.get(
           f'/api/projects/{project_id}/decisions/search',
           params={'q': query}
       )
       return response.status_code, response.elapsed.total_seconds()

   async def load_test_concurrent_searches(num_requests=100):
       """Test concurrent search requests."""
       queries = [
           'foundation changes',
           'HVAC system',
           'structural modifications',
           'landscape design',
           'interior finishes'
       ]

       start = time.time()

       # Execute concurrent requests
       tasks = [
           simulate_search(random.choice(queries))
           for _ in range(num_requests)
       ]

       results = await asyncio.gather(*tasks)

       duration = time.time() - start

       # Analyze results
       success_count = sum(1 for status, _ in results if status == 200)
       latencies = [latency for _, latency in results]

       print(f"\n=== Load Test Results ===")
       print(f"Total Requests: {num_requests}")
       print(f"Success Rate: {success_count/num_requests*100:.1f}%")
       print(f"Total Duration: {duration:.2f}s")
       print(f"Requests/sec: {num_requests/duration:.1f}")
       print(f"Avg Latency: {sum(latencies)/len(latencies)*1000:.0f}ms")
       print(f"Max Latency: {max(latencies)*1000:.0f}ms")

       assert success_count == num_requests, "Some requests failed"
       assert sum(latencies)/len(latencies) < 1.0, "Avg latency >1s"

   if __name__ == '__main__':
       asyncio.run(load_test_concurrent_searches(100))
   ```

2. **Run load test**
   - 100 concurrent requests
   - Verify <1s average latency
   - Check no errors

---

## Dev Notes

### IVFFLAT Index Configuration

**Parameters:**
```sql
CREATE INDEX idx_decisions_embedding_cosine
ON decisions
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

| Parameter | Value | Notes |
|-----------|-------|-------|
| `lists` | 100 | Number of clusters (√total_rows) |
| `probes` | 1 (default) | Number of clusters to search (runtime param) |

**When to adjust:**
- <1000 decisions: `lists = 32`
- 1000-10000 decisions: `lists = 100` (MVP target)
- 10000-100000 decisions: `lists = 316`
- >100000 decisions: Consider HNSW index

### Query Optimization Checklist

- [ ] Index exists and is used (check EXPLAIN)
- [ ] `ANALYZE decisions` run after data load
- [ ] Filter by `project_id` before vector search
- [ ] Filter by `embedding IS NOT NULL`
- [ ] Use `<=>` operator (not distance functions)
- [ ] Limit results (LIMIT clause)

### Performance Benchmarks

| Dataset Size | No Index | IVFFLAT | HNSW (Phase 2) |
|-------------|----------|---------|----------------|
| 100 | ~50ms | ~20ms | ~10ms |
| 1,000 | ~300ms | ~100ms | ~30ms |
| 10,000 | ~3s | ~500ms | ~100ms |
| 100,000 | ~30s | ~2s | ~300ms |

### Caching Strategy

**What to cache:**
1. Query embeddings (most expensive operation)
2. Popular search queries (top 100)
3. Project-level statistics

**Cache invalidation:**
- Query embeddings: Never expire (deterministic)
- Statistics: Expire after new decisions added
- Use LRU eviction policy

---

## File List

**Modified/Created:**
- `alembic/versions/003_add_ivfflat_index.py` (new) - IVFFLAT index migration
- `app/utils/query_cache.py` (new) - Query caching utilities
- `app/api/decisions.py` (modify) - Add caching and monitoring
- `scripts/analyze_query_performance.py` (new) - Query analysis tool
- `tests/load/test_concurrent_searches.py` (new) - Load tests
- `tests/performance/test_search_optimization.py` (new) - Performance tests
- `docs/performance/tuning_guide.md` (new) - Performance tuning documentation

---

## Testing Strategy

### Performance Tests

```python
@pytest.mark.performance
@pytest.mark.asyncio
async def test_search_performance_1000_decisions():
    """Test search performance with 1000 decisions."""
    import time

    db = Session()

    # Ensure 1000+ decisions exist
    decision_count = db.query(func.count(Decision.id)).scalar()
    assert decision_count >= 1000

    # Measure search time
    start = time.time()

    response = await client.get(
        f'/api/projects/{project_id}/decisions/search',
        params={'q': 'foundation changes'}
    )

    duration = time.time() - start

    assert response.status_code == 200
    assert duration < 0.5, f"Search took {duration*1000:.0f}ms, expected <500ms"

@pytest.mark.performance
@pytest.mark.asyncio
async def test_cache_hit_faster_than_miss():
    """Test cached queries are faster."""
    import time

    query = 'unique query string 12345'

    # First request (cache miss)
    start = time.time()
    response1 = await client.get(
        f'/api/projects/{project_id}/decisions/search',
        params={'q': query}
    )
    miss_time = time.time() - start

    # Second request (cache hit)
    start = time.time()
    response2 = await client.get(
        f'/api/projects/{project_id}/decisions/search',
        params={'q': query}
    )
    hit_time = time.time() - start

    assert hit_time < miss_time * 0.5  # Cache hit should be 50%+ faster
```

### Load Tests

```bash
# Run load test
python tests/load/test_concurrent_searches.py

# Expected output:
# === Load Test Results ===
# Total Requests: 100
# Success Rate: 100.0%
# Total Duration: 12.5s
# Requests/sec: 8.0
# Avg Latency: 245ms
# Max Latency: 680ms
```

---

## Change Log

| Date | Change |
|------|--------|
| 2026-02-08 | Created story |

---

**Related Stories:** 2.2 (pgvector), 2.4 (Search API)
**Blocked By:** 2.4 (needs search endpoint)
**Blocks:** None (optimization)
