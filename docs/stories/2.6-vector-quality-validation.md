# Story 2.6: Vector Quality Validation

**Story ID:** 2.6
**Assigned to:** @dev
**Sprint:** Sprint 3 (Week 5)
**Status:** Draft
**Estimation:** 5 story points (2 days)

---

## Summary

Validate that embedding vectors accurately capture semantic similarity between decisions. Create validation scripts and quality metrics to ensure semantic search returns relevant results.

---

## Acceptance Criteria

- [ ] Validation script created: `scripts/validate_embeddings.py`
- [ ] Test known similar decisions have similarity ‚â•0.7
- [ ] Test known dissimilar decisions have similarity <0.5
- [ ] Manual validation: Review 20 similarity pairs
- [ ] Quality report generated with metrics:
  - [ ] Average similarity for known pairs
  - [ ] False positive rate (<10%)
  - [ ] False negative rate (<10%)
- [ ] Edge cases tested: Very short text, very long text, special characters
- [ ] Performance tested: Query time <500ms for 1000 decisions
- [ ] Documentation: Validation results saved to `docs/validation/`
- [ ] Tests passing: `pytest tests/unit/test_vector_quality.py` (80%+ coverage)

---

## Tasks

### Task 1: Create Validation Script (0.75 days)

1. **Create validation script**
   ```python
   # scripts/validate_embeddings.py

   import numpy as np
   from sklearn.metrics.pairwise import cosine_similarity
   from app.database import Session
   from app.database.models import Decision
   from app.utils.embeddings import EmbeddingService

   embedder = EmbeddingService()

   def calculate_similarity(emb1: List[float], emb2: List[float]) -> float:
       """Calculate cosine similarity between two embeddings."""
       return cosine_similarity([emb1], [emb2])[0][0]

   def validate_known_similar_pairs():
       """Test known similar decisions have high similarity."""
       db = Session()

       # Define known similar pairs (manually curated)
       similar_pairs = [
           ('foundation depth change', 'foundation reinforcement'),
           ('HVAC capacity increase', 'HVAC system upgrade'),
           ('facade material selection', 'exterior finish choice')
       ]

       results = []
       for desc1, desc2 in similar_pairs:
           # Find decisions matching descriptions
           d1 = db.query(Decision).filter(
               Decision.statement.ilike(f'%{desc1}%')
           ).first()

           d2 = db.query(Decision).filter(
               Decision.statement.ilike(f'%{desc2}%')
           ).first()

           if not d1 or not d2 or not d1.embedding or not d2.embedding:
               continue

           similarity = calculate_similarity(d1.embedding, d2.embedding)

           results.append({
               'pair': (d1.statement, d2.statement),
               'similarity': similarity,
               'expected': '‚â•0.7',
               'passed': similarity >= 0.7
           })

       return results

   def validate_known_dissimilar_pairs():
       """Test known dissimilar decisions have low similarity."""
       db = Session()

       dissimilar_pairs = [
           ('foundation depth', 'interior color scheme'),
           ('HVAC system', 'landscape tree selection'),
           ('structural reinforcement', 'budget discussion')
       ]

       results = []
       for desc1, desc2 in dissimilar_pairs:
           d1 = db.query(Decision).filter(
               Decision.statement.ilike(f'%{desc1}%')
           ).first()

           d2 = db.query(Decision).filter(
               Decision.statement.ilike(f'%{desc2}%')
           ).first()

           if not d1 or not d2 or not d1.embedding or not d2.embedding:
               continue

           similarity = calculate_similarity(d1.embedding, d2.embedding)

           results.append({
               'pair': (d1.statement, d2.statement),
               'similarity': similarity,
               'expected': '<0.5',
               'passed': similarity < 0.5
           })

       return results

   def generate_validation_report():
       """Generate comprehensive validation report."""
       similar_results = validate_known_similar_pairs()
       dissimilar_results = validate_known_dissimilar_pairs()

       report = {
           'similar_pairs': {
               'total': len(similar_results),
               'passed': sum(1 for r in similar_results if r['passed']),
               'results': similar_results
           },
           'dissimilar_pairs': {
               'total': len(dissimilar_results),
               'passed': sum(1 for r in dissimilar_results if r['passed']),
               'results': dissimilar_results
           }
       }

       # Calculate accuracy
       total_tests = len(similar_results) + len(dissimilar_results)
       total_passed = report['similar_pairs']['passed'] + report['dissimilar_pairs']['passed']
       report['accuracy'] = (total_passed / total_tests) if total_tests > 0 else 0

       return report

   if __name__ == '__main__':
       report = generate_validation_report()

       print("\n=== Embedding Quality Validation Report ===\n")

       print(f"Similar Pairs: {report['similar_pairs']['passed']}/{report['similar_pairs']['total']} passed")
       for r in report['similar_pairs']['results']:
           status = '‚úÖ' if r['passed'] else '‚ùå'
           print(f"  {status} {r['pair'][0][:40]} <-> {r['pair'][1][:40]}")
           print(f"     Similarity: {r['similarity']:.3f} (expected {r['expected']})")

       print(f"\nDissimilar Pairs: {report['dissimilar_pairs']['passed']}/{report['dissimilar_pairs']['total']} passed")
       for r in report['dissimilar_pairs']['results']:
           status = '‚úÖ' if r['passed'] else '‚ùå'
           print(f"  {status} {r['pair'][0][:40]} <-> {r['pair'][1][:40]}")
           print(f"     Similarity: {r['similarity']:.3f} (expected {r['expected']})")

       print(f"\nüìä Overall Accuracy: {report['accuracy']*100:.1f}%")

       # Save report
       import json
       with open('docs/validation/embedding_validation_report.json', 'w') as f:
           json.dump(report, f, indent=2, default=str)

       assert report['accuracy'] >= 0.9, f"Accuracy {report['accuracy']*100:.1f}% below 90% threshold"
   ```

2. **Run validation**
   - Execute script on test dataset
   - Review results
   - Adjust if needed

### Task 2: Manual Quality Review (0.5 days)

1. **Create manual review script**
   ```python
   # scripts/manual_review_similarity.py

   def manual_similarity_review(sample_size=20):
       """Manually review similarity results."""
       db = Session()

       # Get random decision
       base_decision = db.query(Decision).order_by(func.random()).first()

       print(f"\nBase Decision: {base_decision.statement}")
       print(f"Why: {base_decision.why}")

       # Find top 10 similar decisions
       sql = text("""
           SELECT id, decision_statement, why,
                  1 - (embedding <=> :query_emb::vector) as similarity
           FROM decisions
           WHERE id != :base_id
               AND embedding IS NOT NULL
           ORDER BY embedding <=> :query_emb::vector
           LIMIT 10
       """)

       results = db.execute(sql, {
           'query_emb': base_decision.embedding,
           'base_id': base_decision.id
       }).fetchall()

       relevant_count = 0
       for i, row in enumerate(results, 1):
           print(f"\n--- Similar Decision {i} ---")
           print(f"Statement: {row.decision_statement}")
           print(f"Why: {row.why}")
           print(f"Similarity: {row.similarity:.3f}")

           user_input = input("Is this relevant/similar? (y/n): ").lower()
           if user_input == 'y':
               relevant_count += 1

       precision = relevant_count / len(results)
       print(f"\n‚úÖ Precision@10: {precision*100:.1f}% ({relevant_count}/10 relevant)")

       return precision

   if __name__ == '__main__':
       precisions = []
       for i in range(5):  # Review 5 random samples
           print(f"\n{'='*60}")
           print(f"Sample {i+1}/5")
           print('='*60)
           precision = manual_similarity_review()
           precisions.append(precision)

       avg_precision = sum(precisions) / len(precisions)
       print(f"\nüìä Average Precision@10: {avg_precision*100:.1f}%")
   ```

2. **Execute manual review**
   - Review 20 similarity pairs
   - Record precision metrics
   - Document findings

### Task 3: Test Edge Cases (0.5 days)

1. **Create edge case tests**
   ```python
   # tests/unit/test_vector_edge_cases.py

   @pytest.mark.asyncio
   async def test_very_short_text():
       """Test embedding works with very short text."""
       short_text = "Yes"
       embedding = embedder.embed_text(short_text)

       assert len(embedding) == 384
       assert all(isinstance(x, float) for x in embedding)

   @pytest.mark.asyncio
   async def test_very_long_text():
       """Test embedding handles truncation of long text."""
       long_text = "x" * 5000  # Very long
       embedding = embedder.embed_text(long_text)

       assert len(embedding) == 384  # Should still work

   @pytest.mark.asyncio
   async def test_special_characters():
       """Test embedding handles special characters."""
       text = "Decision: Changed material @ $50K (10% increase) üèóÔ∏è"
       embedding = embedder.embed_text(text)

       assert len(embedding) == 384

   @pytest.mark.asyncio
   async def test_unicode_text():
       """Test embedding handles Unicode."""
       text = "Decis√£o sobre funda√ß√£o: mudan√ßa de profundidade"
       embedding = embedder.embed_text(text)

       assert len(embedding) == 384

   @pytest.mark.asyncio
   async def test_html_tags():
       """Test embedding handles HTML content."""
       text = "<strong>Decision:</strong> Changed design <br> New approach"
       embedding = embedder.embed_text(text)

       assert len(embedding) == 384
   ```

2. **Run edge case tests**
   - `pytest tests/unit/test_vector_edge_cases.py`
   - Verify all pass

### Task 4: Performance Validation (0.5 days)

1. **Create performance test**
   ```python
   # tests/performance/test_vector_search_performance.py

   @pytest.mark.performance
   @pytest.mark.asyncio
   async def test_vector_search_latency():
       """Test vector search performance at scale."""
       import time

       db = Session()

       # Ensure we have 1000+ decisions with embeddings
       decision_count = db.query(func.count(Decision.id)).filter(
           Decision.embedding.isnot(None)
       ).scalar()

       assert decision_count >= 1000, f"Need 1000+ decisions, found {decision_count}"

       # Generate random query embedding
       query_emb = [random.random() for _ in range(384)]

       # Measure query time
       start = time.time()

       sql = text("""
           SELECT id, decision_statement,
                  1 - (embedding <=> :query_emb::vector) as similarity
           FROM decisions
           WHERE embedding IS NOT NULL
           ORDER BY embedding <=> :query_emb::vector
           LIMIT 10
       """)

       results = db.execute(sql, {'query_emb': query_emb}).fetchall()
       duration = time.time() - start

       print(f"\nüìä Vector search latency: {duration*1000:.0f}ms")
       print(f"   Searched: {decision_count} decisions")
       print(f"   Returned: {len(results)} results")

       assert duration < 0.5, f"Query took {duration*1000:.0f}ms, expected <500ms"
       assert len(results) == 10
   ```

2. **Run performance test**
   - Create test dataset with 1000+ decisions
   - Measure query latency
   - Verify <500ms

### Task 5: Generate Quality Metrics Report (0.25 days)

1. **Create metrics calculation**
   ```python
   # scripts/generate_quality_metrics.py

   def calculate_quality_metrics():
       """Calculate comprehensive quality metrics."""
       db = Session()

       # Get all decisions with embeddings
       decisions = db.query(Decision).filter(
           Decision.embedding.isnot(None)
       ).all()

       metrics = {
           'total_decisions': len(decisions),
           'decisions_with_embeddings': len(decisions),
           'embedding_coverage': 1.0 if len(decisions) > 0 else 0,
           'avg_embedding_norm': np.mean([
               np.linalg.norm(d.embedding) for d in decisions
           ]),
           'embedding_dimension': 384
       }

       # Sample similarity distribution
       sample_size = min(100, len(decisions))
       similarities = []

       for _ in range(sample_size):
           d1, d2 = random.sample(decisions, 2)
           sim = cosine_similarity([d1.embedding], [d2.embedding])[0][0]
           similarities.append(sim)

       metrics['similarity_stats'] = {
           'mean': float(np.mean(similarities)),
           'std': float(np.std(similarities)),
           'min': float(np.min(similarities)),
           'max': float(np.max(similarities))
       }

       return metrics

   if __name__ == '__main__':
       metrics = calculate_quality_metrics()

       print("\n=== Embedding Quality Metrics ===\n")
       for key, value in metrics.items():
           print(f"{key}: {value}")

       # Save report
       with open('docs/validation/quality_metrics.json', 'w') as f:
           json.dump(metrics, f, indent=2)
   ```

2. **Generate and save report**
   - Run metrics calculation
   - Save to `docs/validation/`

---

## Dev Notes

### Validation Approach

**Automated Tests:**
1. Known similar pairs (high similarity expected)
2. Known dissimilar pairs (low similarity expected)
3. Edge cases (short, long, special chars)
4. Performance benchmarks

**Manual Review:**
1. Sample 20 random similarity queries
2. Human judge relevance
3. Calculate precision@10
4. Target: ‚â•80% precision

### Quality Metrics

```json
{
  "total_decisions": 127,
  "decisions_with_embeddings": 127,
  "embedding_coverage": 1.0,
  "avg_embedding_norm": 15.4,
  "embedding_dimension": 384,
  "similarity_stats": {
    "mean": 0.42,
    "std": 0.18,
    "min": 0.03,
    "max": 0.98
  },
  "validation_results": {
    "similar_pairs_accuracy": 0.95,
    "dissimilar_pairs_accuracy": 0.90,
    "overall_accuracy": 0.92
  },
  "manual_review": {
    "samples": 5,
    "avg_precision_at_10": 0.84
  }
}
```

### Example Similar Pairs

| Decision A | Decision B | Similarity | Expected |
|-----------|-----------|------------|----------|
| "Changed foundation depth" | "Reinforced foundation" | 0.85 | ‚â•0.7 ‚úÖ |
| "Increased HVAC capacity" | "Upgraded HVAC system" | 0.79 | ‚â•0.7 ‚úÖ |
| "Selected facade material" | "Chose exterior finish" | 0.82 | ‚â•0.7 ‚úÖ |

### Example Dissimilar Pairs

| Decision A | Decision B | Similarity | Expected |
|-----------|-----------|------------|----------|
| "Foundation depth change" | "Interior color scheme" | 0.23 | <0.5 ‚úÖ |
| "HVAC system upgrade" | "Landscape tree selection" | 0.31 | <0.5 ‚úÖ |
| "Structural reinforcement" | "Budget discussion" | 0.18 | <0.5 ‚úÖ |

---

## File List

**Modified/Created:**
- `scripts/validate_embeddings.py` (new) - Automated validation script
- `scripts/manual_review_similarity.py` (new) - Manual review tool
- `scripts/generate_quality_metrics.py` (new) - Metrics calculation
- `tests/unit/test_vector_quality.py` (new) - Quality unit tests
- `tests/unit/test_vector_edge_cases.py` (new) - Edge case tests
- `tests/performance/test_vector_search_performance.py` (new) - Performance tests
- `docs/validation/embedding_validation_report.json` (new) - Validation results
- `docs/validation/quality_metrics.json` (new) - Quality metrics

---

## Testing Strategy

### Automated Validation

```python
@pytest.mark.asyncio
async def test_similar_decisions_high_similarity():
    """Test known similar decisions have high similarity."""
    d1 = create_decision("Changed foundation depth", "Soil analysis")
    d2 = create_decision("Reinforced foundation", "Load calculations")

    emb1 = embedder.embed_text(f"{d1.statement} {d1.why}")
    emb2 = embedder.embed_text(f"{d2.statement} {d2.why}")

    similarity = cosine_similarity([emb1], [emb2])[0][0]

    assert similarity >= 0.7, f"Similarity {similarity:.3f} below threshold"

@pytest.mark.asyncio
async def test_dissimilar_decisions_low_similarity():
    """Test known dissimilar decisions have low similarity."""
    d1 = create_decision("Foundation depth", "Structural")
    d2 = create_decision("Interior colors", "Aesthetic preference")

    emb1 = embedder.embed_text(f"{d1.statement} {d1.why}")
    emb2 = embedder.embed_text(f"{d2.statement} {d2.why}")

    similarity = cosine_similarity([emb1], [emb2])[0][0]

    assert similarity < 0.5, f"Similarity {similarity:.3f} above threshold"
```

---

## Change Log

| Date | Change |
|------|--------|
| 2026-02-08 | Created story |

---

**Related Stories:** 2.1 (Embeddings), 2.3 (Pipeline), 2.4 (Search)
**Blocked By:** 2.3 (needs embeddings in DB)
**Blocks:** None (quality validation)
