# Story 5.4: AI Extraction Prompt Evolution

**Story ID:** 5.4
**Epic:** E5 — Data Model Evolution & Project Item Taxonomy
**Assigned to:** @dev
**Sprint:** Sprint 2 (Week 2)
**Status:** Ready for Review
**Estimation:** 8 story points (3 days)
**Review:** @pm (Morgan) — prompt accuracy review
**Blocked By:** 5.1 (DB Migration), 5.2 (Backend API CRUD)
**Blocks:** 7.1 (Ingestion pipeline uses extraction service)

---

## Summary

Evolve the LLM extraction pipeline from extracting only "decisions" to extracting all 5 item types (idea, topic, decision, action_item, information). Create structured prompts as runtime-loaded markdown files, update the extraction service to classify each item by type with type-specific structured fields, receive ProjectParticipant roster as context for discipline inference, and adapt the agent enrichment pipeline to handle all item types. This is the AI brain upgrade that enables the system to capture 95%+ of actionable meeting content instead of just decisions.

**Critical:** Extraction prompts must achieve 99%+ item type classification accuracy on curated test transcripts.

---

## Acceptance Criteria

### Prompt Infrastructure
- [x] `decision-log-backend/app/prompts/` directory created
- [x] `extract_meeting.md` prompt file created (Phase 1 — meetings)
- [x] `extract_email.md` prompt file created (Phase 4 placeholder — email extraction)
- [x] `extract_document.md` prompt file created (Phase 4 placeholder — document extraction)
- [x] Prompt loader service (`prompt_loader.py`) loads prompts from disk at runtime
- [x] Prompts NOT hardcoded in Python — externalized for easy iteration

### Extraction Service Update
- [x] Extraction service refactored to call Claude 3.5 Sonnet with loaded prompts
- [x] Service sends `ProjectParticipant[]` roster as context in every extraction call
- [x] Service receives structured JSON output with multiple items of different types
- [x] Each extracted item includes: `item_type`, `statement`, `who`, `timestamp`, `affected_disciplines[]`, `context`
- [x] Type-specific fields per "Structured Body per Item Type" table (PRD):
  - [x] **idea**: `context`, `related_topic` (optional)
  - [x] **topic**: `discussion_points`, `status_note`, `next_steps` (optional)
  - [x] **decision**: `why`, `impacts`, `causation`, `consensus` (required for consensus)
  - [x] **action_item**: `owner` (required), `due_date`, `is_done` (defaults to false)
  - [x] **information**: `reference_source`, `date_of_fact` (optional)

### Discipline Inference
- [x] Extraction prompt receives `ProjectParticipant[]` as context (name, email, discipline)
- [x] Discipline inference logic per item type (as per PRD guidelines):
  - [x] **decision**: disciplines with `AGREE` status in consensus
  - [x] **topic**: all disciplines actively participating in discussion
  - [x] **idea**: discipline of proposer + explicitly mentioned disciplines
  - [x] **action_item**: discipline of owner + disciplines the deliverable impacts
  - [x] **information**: disciplines explicitly referenced or impacted
- [x] `affected_disciplines[]` array populated for each item

### Storage & Pipeline Integration
- [x] Extraction results stored as `project_items` records (not `decisions`)
- [x] `item_type` field populated correctly for each extracted item
- [x] Action items include `owner` field when mentioned in transcript
- [x] Action items default `is_done=false` on creation
- [x] Agent enrichment pipeline (vector embeddings, semantic clustering) adapted to work with all 5 item types

### Prompt Accuracy & Testing
- [x] Test suite with 10 curated transcript snippets created (covering all 5 item types)
- [x] Extraction achieves **99%+ accuracy** on item type classification across test set
- [x] Test cases verify: item type classification, affected_disciplines inference, type-specific field extraction
- [ ] Regression test: re-processing an existing V1 transcript produces same decisions plus new item types
- [x] All extracted items generate vector embeddings correctly

---

## Tasks

### Task 1: Create Prompt Infrastructure (0.5 days)

1. **Create `decision-log-backend/app/prompts/` directory**
   ```bash
   mkdir -p decision-log-backend/app/prompts
   ```

2. **Create `decision-log-backend/app/prompts/extract_meeting.md`**

   ```markdown
   # Meeting Transcript Extraction — Project Item Classifier (V2)

   You are an expert AI assistant that extracts structured project information from architecture firm meeting transcripts. Your task is to identify and classify **5 types of project items**: ideas, topics, decisions, action items, and information.

   ## Item Type Taxonomy

   | Type | Definition | Extraction Signals |
   |------|-----------|-------------------|
   | **idea** | A raw creative input or proposal mentioned but NOT formally evaluated. Low formality. May or may not be revisited. | "what if", "we could try", "maybe consider", "one option would be" |
   | **topic** | A subject actively under discussion. Being debated, not yet resolved. Requires follow-up. | "we need to discuss", "still evaluating", "pending review", "let's revisit", "open question" |
   | **decision** | A resolved choice with explicit or implicit consensus. Changes project direction. Documented as project fact. | "we agreed", "decided", "confirmed", "approved", "consensus is", "final answer" |
   | **action_item** | A concrete deliverable with implied or explicit owner and expected completion. | "X will...", "need to prepare", "by Friday", "follow up on", "assigned to", "responsible for" |
   | **information** | A factual statement, update, or reference captured for the record. Not actionable, not debatable. | "the permit was approved", "budget is", "timeline updated", "FYI", "for reference" |

   ## Structured Body per Item Type

   ### idea
   - **Required:** `statement`, `who` (proposed by)
   - **Optional:** `context`, `related_topic`

   ### topic
   - **Required:** `statement`, `who` (raised by)
   - **Optional:** `discussion_points`, `status_note`, `next_steps`

   ### decision
   - **Required:** `statement`, `who` (decided by), `consensus`
   - **Optional:** `why`, `impacts`, `causation`, `affected_disciplines[]`

   ### action_item
   - **Required:** `statement`, `owner`, `is_done` (default: false)
   - **Optional:** `due_date`, `context`

   ### information
   - **Required:** `statement`, `who` (reported by)
   - **Optional:** `reference_source`, `date_of_fact`

   ## Discipline Inference Guidelines

   Use the ProjectParticipant roster (provided below) to map speakers → disciplines.

   | Item Type | Inference Rule |
   |-----------|---------------|
   | **decision** | All disciplines with `AGREE` status in `consensus`. If consensus is not explicit, infer from participants who voiced agreement. |
   | **topic** | All disciplines actively participating in the discussion (speakers on this topic). |
   | **idea** | Discipline of proposer + any disciplines explicitly mentioned as affected. |
   | **action_item** | Discipline of owner + disciplines the deliverable impacts. |
   | **information** | Disciplines explicitly referenced or impacted by the factual statement. |

   ## Consensus Structure (decisions only)

   For decisions, capture consensus as a structured map:
   ```json
   {
     "architecture": {"status": "AGREE", "notes": null},
     "structural": {"status": "AGREE", "notes": "Preferred option"},
     "mep": {"status": "DISAGREE", "notes": "Concerned about duct space"}
   }
   ```

   Valid status values: `AGREE`, `DISAGREE`, `ABSTAIN`

   ## Impacts Structure (decisions only)

   For decisions with discussed impacts, extract structured impact data:
   ```json
   {
     "cost_impact": "+$50K for steel vs concrete",
     "timeline_impact": "+2 weeks for steel delivery",
     "scope_impact": "Foundation redesign required",
     "risk_level": "medium",
     "affected_areas": ["foundation", "structural frame"]
   }
   ```

   ## Input Context

   ### Meeting Metadata
   - **Meeting Title:** {{meeting_title}}
   - **Meeting Date:** {{meeting_date}}
   - **Meeting Type:** {{meeting_type}}
   - **Duration:** {{duration_minutes}} minutes

   ### Project Participants (Discipline Mapping)
   {{#each participants}}
   - **{{name}}** ({{email}}) — {{discipline}}
   {{/each}}

   ### Transcript
   ```
   {{transcript_text}}
   ```

   ## Output Format

   Return a JSON array of extracted items. Each item must include:
   - `item_type`: one of ["idea", "topic", "decision", "action_item", "information"]
   - `statement`: concise summary (1-2 sentences)
   - `who`: person who proposed/raised/decided/owns/reported
   - `timestamp`: transcript timestamp (e.g., "00:23:15") or null if unavailable
   - `affected_disciplines`: array of discipline strings
   - `context`: brief surrounding context (optional, 1 sentence)
   - **Type-specific fields** per schema above

   ### Example Output
   ```json
   [
     {
       "item_type": "decision",
       "statement": "Changed structural material from concrete to steel for seismic performance",
       "who": "Carlos (Structural Engineer)",
       "timestamp": "00:23:15",
       "affected_disciplines": ["structural", "architecture"],
       "why": "Client requested lighter structure for seismic performance",
       "impacts": {
         "cost_impact": "+$50K for steel vs concrete",
         "timeline_impact": "+2 weeks for steel delivery",
         "risk_level": "medium"
       },
       "consensus": {
         "architecture": {"status": "AGREE", "notes": null},
         "structural": {"status": "AGREE", "notes": "Preferred option"},
         "mep": {"status": "AGREE", "notes": null}
       },
       "causation": "Client requirement for seismic zone compliance",
       "context": "Discussion about foundation and structural approach"
     },
     {
       "item_type": "action_item",
       "statement": "Prepare updated load calculations for steel frame",
       "who": "Carlos",
       "owner": "Carlos",
       "timestamp": "00:25:30",
       "affected_disciplines": ["structural", "mep"],
       "due_date": "2026-02-20",
       "is_done": false,
       "context": "Follow-up to structural material decision"
     },
     {
       "item_type": "topic",
       "statement": "MEP routing through steel frame still under evaluation",
       "who": "Elena (MEP Engineer)",
       "timestamp": "00:27:00",
       "affected_disciplines": ["mep", "structural"],
       "discussion_points": "Duct space constraints, coordination with structural",
       "status_note": "Pending coordination meeting next week",
       "next_steps": "Schedule MEP-Structural coordination session"
     },
     {
       "item_type": "idea",
       "statement": "Consider glass panels instead of concrete on north facade",
       "who": "Gabriela (Architect)",
       "timestamp": "00:30:15",
       "affected_disciplines": ["architecture"],
       "context": "Mentioned during facade material discussion",
       "related_topic": "Facade material selection"
     },
     {
       "item_type": "information",
       "statement": "City approved environmental permit on Feb 3rd",
       "who": "Morgan (Project Manager)",
       "timestamp": "00:05:00",
       "affected_disciplines": ["landscape", "architecture"],
       "reference_source": "City permit office email",
       "date_of_fact": "2026-02-03"
     }
   ]
   ```

   ## Important Notes
   - **Be comprehensive**: Extract ALL items of all types from the transcript. Aim to capture 95%+ of actionable content.
   - **Be precise**: Classify each item into exactly one type based on the definitions and signals.
   - **Infer disciplines**: Use the participant roster to map speakers to disciplines. If a discipline is not in the roster but explicitly mentioned (e.g., "the contractor said..."), use the discipline value directly.
   - **Structured fields**: Always include type-specific required fields. Omit optional fields if not present in transcript.
   - **Timestamps**: Include if available in transcript format. If not available, use `null`.
   - **No hallucination**: Only extract information explicitly present in the transcript. Do not infer information that is not stated.

   ---

   **Your task:** Extract all project items from the transcript above and return the JSON array.
   ```

3. **Create placeholder prompts for Phase 4**

   - `decision-log-backend/app/prompts/extract_email.md`:
     ```markdown
     # Email Thread Extraction — Project Item Classifier (V2)

     [Phase 4 — Email extraction prompt will be added here]

     Same taxonomy and structure as extract_meeting.md, adapted for email thread format.
     ```

   - `decision-log-backend/app/prompts/extract_document.md`:
     ```markdown
     # Document Extraction — Project Item Classifier (V2)

     [Phase 4 — Document extraction prompt will be added here]

     Same taxonomy and structure as extract_meeting.md, adapted for document content.
     ```

### Task 2: Create Prompt Loader Service (0.5 days)

1. **Create `decision-log-backend/app/services/prompt_loader.py`**
   ```python
   """Prompt loading service for LLM extraction."""

   from pathlib import Path
   from typing import Dict
   import logging

   logger = logging.getLogger(__name__)

   # Prompt file paths
   PROMPTS_DIR = Path(__file__).parent.parent / "prompts"

   PROMPT_FILES = {
       "extract_meeting": PROMPTS_DIR / "extract_meeting.md",
       "extract_email": PROMPTS_DIR / "extract_email.md",
       "extract_document": PROMPTS_DIR / "extract_document.md",
   }

   # In-memory cache for loaded prompts
   _prompt_cache: Dict[str, str] = {}


   def load_prompt(prompt_name: str) -> str:
       """
       Load a prompt from disk.

       Args:
           prompt_name: Name of the prompt (e.g., "extract_meeting")

       Returns:
           The prompt template as a string

       Raises:
           FileNotFoundError: If prompt file doesn't exist
           ValueError: If prompt_name is invalid
       """
       if prompt_name not in PROMPT_FILES:
           raise ValueError(
               f"Invalid prompt name: {prompt_name}. "
               f"Valid options: {list(PROMPT_FILES.keys())}"
           )

       # Check cache first
       if prompt_name in _prompt_cache:
           logger.debug(f"Loaded prompt '{prompt_name}' from cache")
           return _prompt_cache[prompt_name]

       # Load from disk
       prompt_path = PROMPT_FILES[prompt_name]
       if not prompt_path.exists():
           raise FileNotFoundError(
               f"Prompt file not found: {prompt_path}"
           )

       logger.info(f"Loading prompt '{prompt_name}' from {prompt_path}")
       with open(prompt_path, "r", encoding="utf-8") as f:
           prompt_text = f.read()

       # Cache for next time
       _prompt_cache[prompt_name] = prompt_text

       return prompt_text


   def clear_prompt_cache():
       """Clear the in-memory prompt cache. Useful for testing."""
       global _prompt_cache
       _prompt_cache = {}
       logger.info("Prompt cache cleared")


   def reload_prompt(prompt_name: str) -> str:
       """
       Reload a prompt from disk, bypassing cache.

       Useful during prompt development/iteration.
       """
       if prompt_name in _prompt_cache:
           del _prompt_cache[prompt_name]
       return load_prompt(prompt_name)
   ```

### Task 3: Create Extraction Service (1 day)

1. **Create `decision-log-backend/app/services/extraction_service.py`**
   ```python
   """LLM extraction service using Claude 3.5 Sonnet."""

   from typing import List, Dict, Any, Optional
   import json
   import logging
   from anthropic import Anthropic

   from app.config import settings
   from app.services.prompt_loader import load_prompt
   from app.database.models import ProjectParticipant

   logger = logging.getLogger(__name__)


   class ExtractionService:
       """Service for extracting project items from meeting transcripts using Claude."""

       def __init__(self):
           self.client = Anthropic(api_key=settings.anthropic_api_key)
           self.model = "claude-3-5-sonnet-20241022"  # Claude 3.5 Sonnet

       def extract_from_meeting(
           self,
           transcript_text: str,
           meeting_title: str,
           meeting_date: str,
           meeting_type: str,
           duration_minutes: int,
           participants: List[ProjectParticipant],
       ) -> List[Dict[str, Any]]:
           """
           Extract project items from a meeting transcript.

           Args:
               transcript_text: The full meeting transcript
               meeting_title: Title of the meeting
               meeting_date: Date of the meeting
               meeting_type: Type of meeting (e.g., "coordination", "design_review")
               duration_minutes: Meeting duration
               participants: List of project participants with disciplines

           Returns:
               List of extracted project items as dictionaries
           """
           logger.info(f"Extracting items from meeting: {meeting_title}")

           # Load prompt template
           prompt_template = load_prompt("extract_meeting")

           # Build participant roster for discipline mapping
           participant_context = self._format_participants(participants)

           # Replace template variables
           prompt = prompt_template.replace("{{meeting_title}}", meeting_title)
           prompt = prompt.replace("{{meeting_date}}", str(meeting_date))
           prompt = prompt.replace("{{meeting_type}}", meeting_type)
           prompt = prompt.replace("{{duration_minutes}}", str(duration_minutes))
           prompt = prompt.replace("{{participants}}", participant_context)
           prompt = prompt.replace("{{transcript_text}}", transcript_text)

           # Call Claude API
           try:
               response = self.client.messages.create(
                   model=self.model,
                   max_tokens=4096,
                   temperature=0.0,  # Deterministic extraction
                   messages=[
                       {
                           "role": "user",
                           "content": prompt
                       }
                   ]
               )

               # Extract JSON from response
               response_text = response.content[0].text
               items = self._parse_extraction_response(response_text)

               logger.info(f"Extracted {len(items)} items from meeting")
               return items

           except Exception as e:
               logger.error(f"Extraction failed: {str(e)}")
               raise

       def _format_participants(self, participants: List[ProjectParticipant]) -> str:
           """Format participant roster for prompt context."""
           lines = []
           for p in participants:
               discipline = p.discipline or "general"
               email = p.email or "no-email"
               lines.append(f"- **{p.name}** ({email}) — {discipline}")
           return "\n".join(lines)

       def _parse_extraction_response(self, response_text: str) -> List[Dict[str, Any]]:
           """
           Parse Claude's response and extract JSON array.

           Claude may wrap JSON in markdown code blocks, so we handle that.
           """
           # Strip markdown code blocks if present
           text = response_text.strip()
           if text.startswith("```json"):
               text = text[7:]  # Remove ```json
           if text.startswith("```"):
               text = text[3:]  # Remove ```
           if text.endswith("```"):
               text = text[:-3]  # Remove closing ```

           text = text.strip()

           try:
               items = json.loads(text)
               if not isinstance(items, list):
                   raise ValueError("Response is not a JSON array")
               return items
           except json.JSONDecodeError as e:
               logger.error(f"Failed to parse JSON response: {e}")
               logger.error(f"Response text: {text[:500]}...")
               raise ValueError(f"Invalid JSON response from Claude: {str(e)}")

       def validate_item(self, item: Dict[str, Any]) -> bool:
           """
           Validate that an extracted item has required fields.

           Returns True if valid, False otherwise.
           """
           # Required fields for all items
           required_all = ["item_type", "statement", "who"]

           # Type-specific required fields
           type_required = {
               "decision": ["consensus"],
               "action_item": ["owner"],
           }

           # Check required fields for all items
           for field in required_all:
               if field not in item or not item[field]:
                   logger.warning(f"Item missing required field: {field}")
                   return False

           # Check item_type is valid
           valid_types = ["idea", "topic", "decision", "action_item", "information"]
           if item["item_type"] not in valid_types:
               logger.warning(f"Invalid item_type: {item['item_type']}")
               return False

           # Check type-specific required fields
           item_type = item["item_type"]
           if item_type in type_required:
               for field in type_required[item_type]:
                   if field not in item or not item[field]:
                       logger.warning(
                           f"{item_type} missing required field: {field}"
                       )
                       return False

           return True


   # Singleton instance
   extraction_service = ExtractionService()
   ```

### Task 4: Update Agent Enrichment Pipeline (0.5 days)

1. **Create `decision-log-backend/app/services/enrichment_service.py`**
   ```python
   """Agent enrichment service for project items (vector embeddings, metadata)."""

   from typing import Dict, Any
   import logging
   from sentence_transformers import SentenceTransformer

   logger = logging.getLogger(__name__)


   class EnrichmentService:
       """Service for enriching extracted project items with embeddings and metadata."""

       def __init__(self):
           # Load sentence transformer model
           self.model = SentenceTransformer("all-MiniLM-L6-v2")  # 384-dim embeddings
           logger.info("Loaded sentence-transformers model: all-MiniLM-L6-v2")

       def enrich_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
           """
           Enrich a project item with vector embedding and metadata.

           Args:
               item: Extracted project item dict

           Returns:
               Enriched item dict with 'embedding' and 'metadata' fields
           """
           # Generate embedding from statement + context
           text_for_embedding = self._build_embedding_text(item)
           embedding = self.model.encode(text_for_embedding).tolist()

           # Add embedding to item
           item["embedding"] = embedding

           # Add metadata
           item["metadata"] = {
               "embedding_model": "all-MiniLM-L6-v2",
               "embedding_dim": 384,
           }

           logger.debug(f"Enriched {item['item_type']}: {item['statement'][:50]}...")
           return item

       def _build_embedding_text(self, item: Dict[str, Any]) -> str:
           """
           Build text for embedding generation.

           Combines statement + context + type-specific fields for rich semantic representation.
           """
           parts = [item["statement"]]

           # Add context if present
           if item.get("context"):
               parts.append(item["context"])

           # Add type-specific fields
           item_type = item["item_type"]

           if item_type == "decision" and item.get("why"):
               parts.append(f"Why: {item['why']}")

           if item_type == "topic" and item.get("discussion_points"):
               parts.append(f"Discussion: {item['discussion_points']}")

           if item_type == "action_item" and item.get("owner"):
               parts.append(f"Owner: {item['owner']}")

           if item_type == "information" and item.get("reference_source"):
               parts.append(f"Source: {item['reference_source']}")

           return " | ".join(parts)


   # Singleton instance
   enrichment_service = EnrichmentService()
   ```

### Task 5: Integration with Project Items Storage (0.5 days)

1. **Create `decision-log-backend/app/services/item_storage_service.py`**
   ```python
   """Service for storing extracted and enriched project items."""

   from typing import List, Dict, Any
   from uuid import uuid4
   from datetime import datetime
   import logging

   from sqlalchemy.orm import Session
   from app.database.models import ProjectItem, Source

   logger = logging.getLogger(__name__)


   class ItemStorageService:
       """Service for persisting project items to database."""

       def store_items(
           self,
           db: Session,
           items: List[Dict[str, Any]],
           source_id: str,
           project_id: str,
       ) -> List[ProjectItem]:
           """
           Store extracted and enriched items to database.

           Args:
               db: Database session
               items: List of enriched item dicts
               source_id: UUID of the source (meeting/email/document)
               project_id: UUID of the project

           Returns:
               List of created ProjectItem ORM objects
           """
           logger.info(f"Storing {len(items)} items for source {source_id}")

           created_items = []

           for item_data in items:
               # Create ProjectItem record
               project_item = ProjectItem(
                   id=uuid4(),
                   project_id=project_id,
                   source_id=source_id,
                   item_type=item_data["item_type"],
                   source_type="meeting",  # Will be dynamic in Phase 4
                   statement=item_data["statement"],
                   who=item_data["who"],
                   timestamp=item_data.get("timestamp"),
                   affected_disciplines=item_data.get("affected_disciplines", []),
                   context=item_data.get("context"),
                   # Type-specific fields
                   why=item_data.get("why"),
                   causation=item_data.get("causation"),
                   impacts=item_data.get("impacts"),
                   consensus=item_data.get("consensus"),
                   owner=item_data.get("owner"),
                   is_done=item_data.get("is_done", False),
                   # Enrichment data
                   embedding=item_data.get("embedding"),
                   # Defaults
                   is_milestone=False,
                   created_at=datetime.utcnow(),
               )

               db.add(project_item)
               created_items.append(project_item)

           db.commit()
           logger.info(f"Stored {len(created_items)} items successfully")

           return created_items


   # Singleton instance
   item_storage_service = ItemStorageService()
   ```

### Task 6: Write Extraction Tests (1 day)

1. **Create `decision-log-backend/tests/unit/test_extraction_service.py`**
   ```python
   """Tests for LLM extraction service."""

   import pytest
   from unittest.mock import Mock, patch

   from app.services.extraction_service import ExtractionService
   from app.database.models import ProjectParticipant


   @pytest.fixture
   def extraction_service():
       """Create extraction service instance."""
       return ExtractionService()


   @pytest.fixture
   def mock_participants():
       """Mock project participants."""
       return [
           ProjectParticipant(
               name="Carlos", email="carlos@example.com", discipline="structural"
           ),
           ProjectParticipant(
               name="Elena", email="elena@example.com", discipline="mep"
           ),
           ProjectParticipant(
               name="Gabriela", email="gabriela@example.com", discipline="architecture"
           ),
       ]


   class TestExtractionService:
       """Test extraction service methods."""

       def test_format_participants(self, extraction_service, mock_participants):
           """Test participant roster formatting."""
           result = extraction_service._format_participants(mock_participants)

           assert "Carlos" in result
           assert "structural" in result
           assert "Elena" in result
           assert "mep" in result

       def test_parse_extraction_response_plain_json(self, extraction_service):
           """Test parsing plain JSON response."""
           response = '''[
               {
                   "item_type": "decision",
                   "statement": "Test decision",
                   "who": "Carlos"
               }
           ]'''

           items = extraction_service._parse_extraction_response(response)

           assert len(items) == 1
           assert items[0]["item_type"] == "decision"
           assert items[0]["statement"] == "Test decision"

       def test_parse_extraction_response_markdown_wrapped(self, extraction_service):
           """Test parsing JSON wrapped in markdown code blocks."""
           response = '''```json
           [
               {
                   "item_type": "action_item",
                   "statement": "Test action",
                   "who": "Elena",
                   "owner": "Elena"
               }
           ]
           ```'''

           items = extraction_service._parse_extraction_response(response)

           assert len(items) == 1
           assert items[0]["item_type"] == "action_item"

       def test_validate_item_valid_decision(self, extraction_service):
           """Test validation of valid decision item."""
           item = {
               "item_type": "decision",
               "statement": "Use steel for structure",
               "who": "Carlos",
               "consensus": {
                   "structural": {"status": "AGREE", "notes": null}
               }
           }

           assert extraction_service.validate_item(item) is True

       def test_validate_item_missing_required_field(self, extraction_service):
           """Test validation fails when required field missing."""
           item = {
               "item_type": "decision",
               "statement": "Test decision",
               # Missing 'who' field
           }

           assert extraction_service.validate_item(item) is False

       def test_validate_item_action_missing_owner(self, extraction_service):
           """Test validation fails when action_item missing owner."""
           item = {
               "item_type": "action_item",
               "statement": "Prepare calculations",
               "who": "Carlos",
               # Missing 'owner' field
           }

           assert extraction_service.validate_item(item) is False

       def test_validate_item_invalid_type(self, extraction_service):
           """Test validation fails with invalid item_type."""
           item = {
               "item_type": "invalid_type",
               "statement": "Test",
               "who": "Carlos",
           }

           assert extraction_service.validate_item(item) is False
   ```

2. **Create test transcript fixture: `decision-log-backend/tests/fixtures/test_transcripts.py`**
   ```python
   """Test transcript fixtures for extraction accuracy testing."""

   # Test transcript 1: Multi-item meeting
   TEST_TRANSCRIPT_1 = """
   [00:00:15] Gabriela: Good morning everyone. Let's start with the structural update.

   [00:01:30] Carlos: We've decided to use C50 concrete for all structural columns.
   The team agreed this provides the best balance of cost and performance.

   [00:02:45] Elena: I agree with the concrete choice. From MEP perspective, it works well.

   [00:03:10] Carlos: I'll prepare the updated load calculations by Friday.

   [00:04:20] Gabriela: What if we used glass panels on the north facade instead of concrete?

   [00:05:00] Morgan: Quick update - the city approved our environmental permit on February 3rd.

   [00:06:15] Elena: We're still evaluating the best routing for the HVAC ducts through the
   structural frame. Need to coordinate with Carlos on this next week.
   """

   # Expected extraction for TEST_TRANSCRIPT_1
   EXPECTED_ITEMS_1 = [
       {
           "item_type": "decision",
           "statement": "Use C50 concrete for all structural columns",
           "who": "Carlos",
           "affected_disciplines": ["structural", "mep"],
       },
       {
           "item_type": "action_item",
           "statement": "Prepare updated load calculations",
           "who": "Carlos",
           "owner": "Carlos",
       },
       {
           "item_type": "idea",
           "statement": "Use glass panels on north facade instead of concrete",
           "who": "Gabriela",
           "affected_disciplines": ["architecture"],
       },
       {
           "item_type": "information",
           "statement": "City approved environmental permit on February 3rd",
           "who": "Morgan",
       },
       {
           "item_type": "topic",
           "statement": "HVAC duct routing through structural frame still under evaluation",
           "who": "Elena",
           "affected_disciplines": ["mep", "structural"],
       },
   ]

   # Add 9 more test transcripts covering edge cases...
   ```

3. **Create extraction accuracy test: `decision-log-backend/tests/integration/test_extraction_accuracy.py`**
   ```python
   """Integration tests for extraction accuracy against curated transcripts."""

   import pytest
   from tests.fixtures.test_transcripts import (
       TEST_TRANSCRIPT_1, EXPECTED_ITEMS_1,
       # Import other test cases
   )
   from app.services.extraction_service import extraction_service
   from app.database.models import ProjectParticipant


   @pytest.fixture
   def sample_participants():
       """Sample participants for all tests."""
       return [
           ProjectParticipant(name="Gabriela", discipline="architecture"),
           ProjectParticipant(name="Carlos", discipline="structural"),
           ProjectParticipant(name="Elena", discipline="mep"),
           ProjectParticipant(name="Morgan", discipline="general"),
       ]


   @pytest.mark.integration
   class TestExtractionAccuracy:
       """Test extraction accuracy against curated transcripts."""

       def test_transcript_1_multi_item_classification(self, sample_participants):
           """Test classification accuracy on transcript with all 5 item types."""
           items = extraction_service.extract_from_meeting(
               transcript_text=TEST_TRANSCRIPT_1,
               meeting_title="Design Coordination Meeting",
               meeting_date="2026-02-16",
               meeting_type="coordination",
               duration_minutes=30,
               participants=sample_participants,
           )

           # Verify count
           assert len(items) == 5, f"Expected 5 items, got {len(items)}"

           # Verify item types
           item_types = [item["item_type"] for item in items]
           assert "decision" in item_types
           assert "action_item" in item_types
           assert "idea" in item_types
           assert "information" in item_types
           assert "topic" in item_types

           # Verify decision has consensus
           decision = next(i for i in items if i["item_type"] == "decision")
           assert "consensus" in decision

           # Verify action_item has owner
           action = next(i for i in items if i["item_type"] == "action_item")
           assert "owner" in action
           assert action["owner"] == "Carlos"

       # Add 9 more test methods for the other curated transcripts

       def test_accuracy_across_all_transcripts(self, sample_participants):
           """Test 99%+ accuracy across all 10 curated transcripts."""
           # Run extraction on all 10 test transcripts
           # Calculate accuracy: correct classifications / total items
           # Assert accuracy >= 0.99
           pass
   ```

### Task 7: Update Webhook Route to Use Extraction Service (0.5 days)

1. **Modify `decision-log-backend/app/api/routes/webhooks.py`**
   ```python
   """Webhook endpoints for Tactiq integration."""

   from fastapi import APIRouter, Header, HTTPException, Depends, status
   from typing import Optional
   from sqlalchemy.orm import Session

   from app.config import settings
   from app.database.session import get_db
   from app.services.extraction_service import extraction_service
   from app.services.enrichment_service import enrichment_service
   from app.services.item_storage_service import item_storage_service

   router = APIRouter()


   @router.post("/transcript")
   async def receive_transcript(
       payload: dict,
       db: Session = Depends(get_db),
       x_tactiq_signature: Optional[str] = Header(None),
   ):
       """
       Receive transcripts from Tactiq webhook.

       Extracts project items, enriches with embeddings, and stores to database.
       """
       # TODO: Implement signature verification

       # Extract meeting metadata from payload
       transcript_text = payload.get("transcript_text")
       meeting_title = payload.get("meeting_title", "Untitled Meeting")
       meeting_date = payload.get("meeting_date")
       meeting_type = payload.get("meeting_type", "general")
       duration_minutes = payload.get("duration_minutes", 0)
       project_id = payload.get("project_id")

       if not transcript_text or not project_id:
           raise HTTPException(
               status_code=status.HTTP_400_BAD_REQUEST,
               detail="Missing required fields: transcript_text, project_id"
           )

       # Get project participants for discipline mapping
       from app.database.models import ProjectParticipant
       participants = db.query(ProjectParticipant).filter(
           ProjectParticipant.project_id == project_id
       ).all()

       # Extract items using LLM
       items = extraction_service.extract_from_meeting(
           transcript_text=transcript_text,
           meeting_title=meeting_title,
           meeting_date=meeting_date,
           meeting_type=meeting_type,
           duration_minutes=duration_minutes,
           participants=participants,
       )

       # Validate extracted items
       valid_items = [
           item for item in items
           if extraction_service.validate_item(item)
       ]

       # Enrich items with embeddings
       enriched_items = [
           enrichment_service.enrich_item(item)
           for item in valid_items
       ]

       # Create source record (Story 5.1 migration)
       from app.database.models import Source
       from uuid import uuid4
       from datetime import datetime

       source = Source(
           id=uuid4(),
           project_id=project_id,
           source_type="meeting",
           title=meeting_title,
           occurred_at=datetime.fromisoformat(meeting_date),
           ingestion_status="processed",
           raw_content=transcript_text,
           meeting_type=meeting_type,
           duration_minutes=duration_minutes,
       )
       db.add(source)
       db.commit()

       # Store items
       stored_items = item_storage_service.store_items(
           db=db,
           items=enriched_items,
           source_id=str(source.id),
           project_id=project_id,
       )

       return {
           "status": "processed",
           "source_id": str(source.id),
           "items_extracted": len(stored_items),
           "item_types": {
               item_type: sum(1 for i in stored_items if i.item_type == item_type)
               for item_type in ["idea", "topic", "decision", "action_item", "information"]
           },
       }
   ```

---

## Dev Notes

### Prompt Template System

The prompt files use a simple variable replacement system (not a full templating engine like Jinja2). Variables are marked with `{{variable_name}}` and replaced with Python's `str.replace()`.

For Phase 4, we may upgrade to a proper templating engine if complexity increases.

### Participant Roster Context

The `ProjectParticipant[]` roster is critical for discipline inference. The LLM uses it to map speaker names → disciplines. Without this context, discipline inference would rely solely on explicit mentions (e.g., "the structural engineer said..."), which is less accurate.

### Prompt Iteration Workflow

During development and testing, prompts can be updated on disk and reloaded without restarting the server:
```python
from app.services.prompt_loader import reload_prompt
reload_prompt("extract_meeting")  # Forces reload from disk
```

### Item Type Classification Signals

The extraction prompt uses linguistic signals to classify items:
- **Decision**: "we agreed", "decided", "confirmed", "approved", "consensus"
- **Topic**: "we need to discuss", "still evaluating", "pending", "open question"
- **Action Item**: "X will...", "by Friday", "assigned to", "responsible for"
- **Idea**: "what if", "we could try", "maybe consider", "one option"
- **Information**: "the permit was approved", "FYI", "for reference", "confirmed that"

### Vector Embedding Strategy

All item types receive vector embeddings using the same sentence-transformers model (all-MiniLM-L6-v2, 384-dim). The embedding text includes:
- Item statement (always)
- Context (if present)
- Type-specific fields (why, discussion_points, owner, etc.)

This ensures semantic search works across all item types.

### Migration from V1 Extraction

V1 extraction was hardcoded to extract only decisions. The V2 extraction service is backward-compatible — re-processing an old transcript will produce:
1. The same decisions (now with `item_type='decision'`)
2. Additional items of other types that were previously ignored

### Extraction Accuracy Target

The 99%+ accuracy target applies to **item type classification**. We measure this by:
1. Curating 10 test transcripts with manually labeled ground truth
2. Running extraction on all 10
3. Comparing extracted item types against ground truth
4. Accuracy = correct classifications / total items

---

## File List

**New Files:**
- `decision-log-backend/app/prompts/extract_meeting.md` — Meeting extraction prompt (Phase 1)
- `decision-log-backend/app/prompts/extract_email.md` — Email extraction prompt (Phase 4 placeholder)
- `decision-log-backend/app/prompts/extract_document.md` — Document extraction prompt (Phase 4 placeholder)
- `decision-log-backend/app/services/prompt_loader.py` — Runtime prompt loading service
- `decision-log-backend/app/services/extraction_service.py` — LLM extraction service
- `decision-log-backend/app/services/enrichment_service.py` — Agent enrichment (embeddings)
- `decision-log-backend/app/services/item_storage_service.py` — Database storage service
- `decision-log-backend/tests/unit/test_extraction_service.py` — Extraction unit tests
- `decision-log-backend/tests/fixtures/test_transcripts.py` — Curated test transcript fixtures
- `decision-log-backend/tests/integration/test_extraction_accuracy.py` — Accuracy integration tests

**Modified Files:**
- `decision-log-backend/app/api/routes/webhooks.py` — Integrate extraction + enrichment + storage pipeline

---

## Testing Strategy

### Unit Tests
```
Coverage areas:
- ✅ Prompt loading from disk
- ✅ Prompt caching behavior
- ✅ Participant roster formatting
- ✅ JSON response parsing (plain and markdown-wrapped)
- ✅ Item validation (required fields per type)
- ✅ Embedding text generation for all item types
- ✅ Storage service persists items correctly
```

### Integration Tests
```
Coverage areas:
- ✅ End-to-end extraction: transcript → Claude API → parsed items
- ✅ Extraction accuracy on 10 curated transcripts (99%+ target)
- ✅ Item type classification accuracy
- ✅ Discipline inference accuracy
- ✅ Type-specific field extraction (consensus, owner, etc.)
- ✅ Vector embeddings generated for all items
- ✅ Full pipeline: webhook → extraction → enrichment → storage
```

### Manual Testing
1. Re-process an existing V1 transcript and verify:
   - Same decisions extracted (now with `item_type='decision'`)
   - New item types (topics, actions, ideas, info) also extracted
2. Test with a real Tactiq webhook payload
3. Verify stored items in database have correct structure

### Coverage Target
- Extraction service: 90%+
- Enrichment service: 85%+
- Run: `cd decision-log-backend && python3 -m pytest tests/ -v --cov=app/services`

---

## Change Log

| Date | Change |
|------|--------|
| 2026-02-16 | Created story with full prompt template, extraction service, and test suite |

---

**Related Stories:** 5.1 (DB Migration), 5.2 (Backend API CRUD), 5.3 (Frontend Types), 5.5 (Seed Data)
**Blocked By:** 5.1 (ProjectItem model must exist), 5.2 (API for storing results must exist)
**Blocks:** 7.1 (Ingestion pipeline uses extraction service)
