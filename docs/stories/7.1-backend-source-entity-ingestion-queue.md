# Story 7.1: Backend — Source Entity & Ingestion Queue

**Story ID:** 7.1
**Epic:** E7 — Multi-Source Ingestion Pipeline
**Assigned to:** @dev
**Sprint:** Sprint 3 (Week 4)
**Status:** Ready for Review
**Estimation:** 8 story points (3 days)
**Review:** @architect (Aria) — ingestion pipeline design

---

## Summary

Build the ingestion approval workflow backend: update the Tactiq webhook to create `Source` records (status: pending) instead of directly processing transcripts, create ingestion API endpoints for listing/approving/rejecting sources, trigger the existing ETL pipeline when a source is approved (passing `ProjectParticipant[]` roster as LLM context), and generate AI one-line summaries for each pending source. This gives Gabriela admin control over what content enters the system.

---

## Acceptance Criteria

### Source Creation from Webhooks
- [x] Tactiq webhook updated: creates a `Source` record with `ingestion_status='pending'`
- [x] Source record populated with: `source_type='meeting'`, `title=meeting_title`, `raw_content=transcript_text`, `occurred_at=meeting_date`, `meeting_type`, `participants`, `duration_minutes`, `webhook_id`
- [x] Webhook returns `202 Accepted` with source_id (not "processed")
- [x] Duplicate webhook detection via `webhook_id` (idempotency)
- [x] AI one-line summary generated from first 2000 chars of transcript via Claude API
- [x] Summary stored in `Source.ai_summary`

### Ingestion API Endpoints
- [x] `GET /api/ingestion` — List sources with pagination and filters
  - [x] Query params: `project_id`, `source_type`, `ingestion_status`, `date_from`, `date_to`, `limit`, `offset`
  - [x] Default filter: `ingestion_status=pending`
  - [x] Response includes: id, project_id, project_name, source_type, title, occurred_at, ingestion_status, ai_summary, meeting_type (if meeting), email_from/subject (if email), file_name/file_type/file_size (if document)
  - [x] Returns `total` count for pagination
- [x] `PATCH /api/ingestion/{source_id}` — Update source status
  - [x] Accepts: `ingestion_status` (approved/rejected)
  - [x] Sets `approved_by` and `approved_at` on approval
  - [x] Admin-only access
- [x] `POST /api/ingestion/batch` — Batch approve/reject
  - [x] Accepts: `source_ids[]`, `action` (approve/reject)
  - [x] Returns count of updated records
  - [x] Admin-only access

### Approval → ETL Pipeline Trigger
- [x] When source status changes to `approved`:
  - [x] Load `ProjectParticipant[]` roster for the source's project
  - [x] Trigger extraction service (Story 5.4) with transcript text + participants
  - [x] Store extracted items as `project_items` with `source_id` linked
  - [x] Update source `ingestion_status` to `processed`
- [x] Pipeline runs asynchronously (background task via FastAPI BackgroundTasks)
- [x] Error handling: if extraction fails, source stays `approved` (not `processed`), error logged

### Pending Count
- [x] `GET /api/ingestion/count` — Returns count of pending sources
  - [x] Used by frontend navigation badge
  - [x] Response: `{"pending": 5}`

### Auth & Access
- [x] All ingestion endpoints require JWT authentication
- [x] Approve/reject restricted to admin/director role
- [x] List endpoint accessible to all authenticated users (they see their projects' sources)

---

## Tasks

### Task 1: Update Webhook Route (0.5 days)

1. **Modify `decision-log-backend/app/api/routes/webhooks.py`**
   ```python
   @router.post("/transcript")
   async def receive_transcript(
       payload: dict,
       background_tasks: BackgroundTasks,
       db: Session = Depends(get_db),
   ):
       # Check for duplicate webhook
       existing = db.query(Source).filter(Source.webhook_id == payload.get("webhook_id")).first()
       if existing:
           return {"status": "duplicate", "source_id": str(existing.id)}

       # Create Source record (pending)
       source = Source(
           id=uuid4(),
           project_id=payload["project_id"],
           source_type="meeting",
           title=payload.get("meeting_title", "Untitled Meeting"),
           occurred_at=datetime.fromisoformat(payload["meeting_date"]),
           ingestion_status="pending",
           raw_content=payload["transcript"],
           meeting_type=payload.get("meeting_type"),
           participants=payload.get("participants"),
           duration_minutes=payload.get("duration_minutes"),
           webhook_id=payload.get("webhook_id"),
       )
       db.add(source)
       db.commit()

       # Generate AI summary in background
       background_tasks.add_task(generate_ai_summary, db, source.id)

       return {"status": "pending", "source_id": str(source.id)}
   ```

### Task 2: Create AI Summary Generator (0.25 days)

1. **Create `decision-log-backend/app/services/summary_service.py`**
   ```python
   async def generate_ai_summary(db: Session, source_id: str):
       """Generate one-line AI summary for a source."""
       source = db.query(Source).get(source_id)
       if not source or not source.raw_content:
           return

       # Use first 2000 chars for summary
       text_preview = source.raw_content[:2000]
       prompt = f"Summarize this meeting transcript in ONE sentence (max 100 words):\n\n{text_preview}"

       client = Anthropic(api_key=settings.anthropic_api_key)
       response = client.messages.create(
           model="claude-3-5-sonnet-20241022",
           max_tokens=150,
           messages=[{"role": "user", "content": prompt}]
       )
       source.ai_summary = response.content[0].text.strip()
       db.commit()
   ```

### Task 3: Create Ingestion Router (1 day)

1. **Create `decision-log-backend/app/api/routes/ingestion.py`**
   - `GET /api/ingestion` with filters and pagination
   - `PATCH /api/ingestion/{source_id}` with approval trigger
   - `POST /api/ingestion/batch` for bulk operations
   - `GET /api/ingestion/count` for pending badge

2. **Approval trigger logic**
   ```python
   @router.patch("/ingestion/{source_id}")
   async def update_source_status(
       source_id: str,
       update: IngestionUpdate,
       background_tasks: BackgroundTasks,
       db: Session = Depends(get_db),
       current_user: User = Depends(require_admin),
   ):
       source = db.query(Source).get(source_id)
       if not source:
           raise HTTPException(404, "Source not found")

       source.ingestion_status = update.ingestion_status
       if update.ingestion_status == "approved":
           source.approved_by = current_user.id
           source.approved_at = datetime.utcnow()
           # Trigger ETL in background
           background_tasks.add_task(process_approved_source, db, source.id)

       db.commit()
       return {"id": str(source.id), "status": source.ingestion_status}
   ```

### Task 4: Create ETL Trigger Service (0.5 days)

1. **Create `decision-log-backend/app/services/ingestion_pipeline.py`**
   ```python
   async def process_approved_source(db: Session, source_id: str):
       """Process an approved source through extraction + enrichment pipeline."""
       source = db.query(Source).get(source_id)
       participants = db.query(ProjectParticipant).filter(
           ProjectParticipant.project_id == source.project_id
       ).all()

       # Extract items
       items = extraction_service.extract_from_meeting(
           transcript_text=source.raw_content,
           meeting_title=source.title,
           meeting_date=str(source.occurred_at),
           meeting_type=source.meeting_type or "general",
           duration_minutes=source.duration_minutes or 0,
           participants=participants,
       )

       # Validate + Enrich + Store
       valid_items = [i for i in items if extraction_service.validate_item(i)]
       enriched = [enrichment_service.enrich_item(i) for i in valid_items]
       item_storage_service.store_items(db, enriched, str(source.id), str(source.project_id))

       # Mark as processed
       source.ingestion_status = "processed"
       db.commit()
   ```

### Task 5: Create Pydantic Models & Tests (0.75 days)

1. **Create `decision-log-backend/app/api/models/ingestion.py`**
   - `IngestionUpdate` — status change request
   - `IngestionBatchAction` — batch approve/reject
   - `SourceListResponse` — paginated list
   - `SourceResponse` — single source detail

2. **Create `decision-log-backend/tests/unit/test_ingestion_api.py`**
   - Test: webhook creates pending source
   - Test: duplicate webhook returns existing
   - Test: GET /ingestion lists pending sources
   - Test: PATCH approves source (admin only)
   - Test: PATCH triggers ETL pipeline
   - Test: Batch approve/reject
   - Test: Pending count endpoint
   - Test: Non-admin cannot approve

---

## Dev Notes

### Ingestion Flow

```
Tactiq Webhook → Source (pending) → AI Summary
                      ↓
Admin reviews in Ingestion Approval page (Story 7.2)
                      ↓
Admin approves → ETL Pipeline → project_items created
                      ↓
Source status = "processed"
```

### API Response Examples

**GET /api/ingestion**
```json
{
  "sources": [
    {
      "id": "uuid",
      "project_id": "uuid",
      "project_name": "Residential Tower Alpha",
      "source_type": "meeting",
      "title": "Foundation Review Meeting",
      "occurred_at": "2026-02-10T14:00:00Z",
      "ingestion_status": "pending",
      "ai_summary": "Team discussed foundation material options and agreed on mat slab approach",
      "meeting_type": "coordination",
      "duration_minutes": 90,
      "created_at": "2026-02-10T15:30:00Z"
    }
  ],
  "total": 5,
  "limit": 50,
  "offset": 0
}
```

---

## File List

**New Files:**
- `decision-log-backend/app/api/routes/ingestion.py` — Ingestion endpoints (GET list, PATCH approve/reject, POST batch, GET count)
- `decision-log-backend/app/api/models/ingestion.py` — Pydantic models (IngestionUpdate, IngestionBatchAction, SourceResponse, SourceListResponse)
- `decision-log-backend/app/services/summary_service.py` — AI summary generation via Claude API (background task)
- `decision-log-backend/app/services/ingestion_pipeline.py` — ETL trigger service (extraction + ProjectItem creation)
- `decision-log-backend/tests/unit/test_ingestion_api.py` — 35 unit tests

**Modified Files:**
- `decision-log-backend/app/api/routes/webhooks.py` — Creates Source record (pending), duplicate detection, schedules AI summary
- `decision-log-backend/app/main.py` — Register ingestion router with `/api` prefix

---

## Testing Strategy

### Unit Tests
```
- ✅ Webhook creates pending Source with raw_content
- ✅ Duplicate webhook detection
- ✅ Ingestion list with filters
- ✅ Approve triggers ETL pipeline
- ✅ Batch approve/reject
- ✅ Pending count
- ✅ Admin-only access enforcement
- ✅ AI summary generation
```

### Coverage Target: 85%+

---

## Change Log

| Date | Change |
|------|--------|
| 2026-02-17 | Created story |
| 2026-02-27 | Implemented all tasks: webhook, summary service, ingestion router, ETL pipeline, Pydantic models, tests (35 passing) |

---

**Related Stories:** 5.1 (Source table), 5.4 (Extraction service), 7.2 (Frontend UI)
**Blocked By:** 5.1 (Source table must exist), 5.4 (Extraction service for ETL trigger)
**Blocks:** 7.2 (Frontend needs these endpoints), 10.1 (Email extraction uses this pipeline)
